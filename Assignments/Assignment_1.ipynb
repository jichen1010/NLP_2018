{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Ji/Documents/Courses/NLP/Assignments'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500 12500 12500 12500\n"
     ]
    }
   ],
   "source": [
    "train_neg_files = glob.glob(\"Assignment_1/aclImdb/train/neg/*.txt\")\n",
    "train_pos_files = glob.glob(\"Assignment_1/aclImdb/train/pos/*.txt\")\n",
    "test_neg_files = glob.glob(\"Assignment_1/aclImdb/test/neg/*.txt\")\n",
    "test_pos_files = glob.glob(\"Assignment_1/aclImdb/test/pos/*.txt\")\n",
    "print(len(train_neg_files), len(train_pos_files), len(test_neg_files), len(test_pos_files))\n",
    "## wow! clean data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read files\n",
    "train_neg_raw = []\n",
    "for i in train_neg_files:\n",
    "    with open(i, 'r') as f:\n",
    "        train_neg_raw.append(f.read())\n",
    "\n",
    "train_pos_raw = []\n",
    "for i in train_pos_files:\n",
    "    with open(i, 'r') as f:\n",
    "        train_pos_raw.append(f.read())\n",
    "\n",
    "test_neg_raw = []\n",
    "for i in test_neg_files:\n",
    "    with open(i, 'r') as f:\n",
    "        test_neg_raw.append(f.read())\n",
    "\n",
    "test_pos_raw = []\n",
    "for i in test_pos_files:\n",
    "    with open(i, 'r') as f:\n",
    "        test_pos_raw.append(f.read())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 322\n"
     ]
    }
   ],
   "source": [
    "## combine into training set and test set\n",
    "from sklearn import model_selection\n",
    "train_data_all = train_pos_raw + train_neg_raw\n",
    "train_label_all = np.append(np.ones(12500), np.zeros(12500))\n",
    "test_data = test_pos_raw + test_neg_raw\n",
    "test_label = train_label_all\n",
    "\n",
    "## finding duplicates and removing them\n",
    "whole, seen, result = train_data_all+test_data, set(), []\n",
    "for idx, item in enumerate(whole):\n",
    "    if item not in seen:\n",
    "        seen.add(item)          # First time seeing the element\n",
    "    else:\n",
    "        result.append(idx)      # Already seen, add the index to the result\n",
    "train_dup_idx = [i for i in result if i < 25000]\n",
    "test_dup_idx = [i-25000 for i in result if i >= 25000]\n",
    "print(len(train_dup_idx), len(test_dup_idx))\n",
    "\n",
    "train_data_all_rmdup = [ item for i,item in enumerate(train_data_all) if i not in train_dup_idx ]\n",
    "train_label_all_rmdup = [ item for i,item in enumerate(train_label_all) if i not in train_dup_idx ]\n",
    "test_data_rmdup = [ item for i,item in enumerate(test_data) if i not in test_dup_idx ]\n",
    "test_label_rmdup = [ item for i,item in enumerate(test_label) if i not in test_dup_idx ]\n",
    "\n",
    "## data split\n",
    "import re\n",
    "for i in range(len(train_data_all_rmdup)):\n",
    "    train_data_all_rmdup[i] = re.sub(\"<br /><br />\", \" \", train_data_all_rmdup[i])\n",
    "    train_data_all_rmdup[i] = re.sub(\"\\'\", \"\", train_data_all_rmdup[i])\n",
    "for i in range(len(test_data_rmdup)):\n",
    "    test_data_rmdup[i] = re.sub(\"<br /><br />\", \" \", test_data_rmdup[i])\n",
    "    test_data_rmdup[i] = re.sub(\"\\'\", \"\", test_data_rmdup[i])\n",
    "\n",
    "train_data, val_data, train_label, val_label = model_selection.train_test_split(train_data_all_rmdup, train_label_all_rmdup, \n",
    "                                                                test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'is', 'looking', 'at', 'buying', 'u.k.', 'startup', 'for', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "  tokens = tokenizer(sent)\n",
    "  return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "\n",
    "# Example\n",
    "tokens = tokenize(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "Tokenizing test data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "# This is the code cell that tokenizes train/val/test datasets\n",
    "# However it takes about 15-20 minutes to run it\n",
    "# For convinience we have provided the preprocessed datasets\n",
    "# Please see the next code cell\n",
    "import pickle as pkl\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "# # val set tokens\n",
    "# print (\"Tokenizing val data\")\n",
    "# val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "# pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# # test set tokens\n",
    "# print (\"Tokenizing test data\")\n",
    "# test_data_tokens, _ = tokenize_dataset(test_data_rmdup)\n",
    "# pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# # train set tokens\n",
    "# print (\"Tokenizing train data\")\n",
    "# train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "# pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 19923\n",
      "Val dataset size is 4981\n",
      "Test dataset size is 24678\n",
      "Total number of tokens in train dataset is 4727320\n"
     ]
    }
   ],
   "source": [
    "# Then, load preprocessed train, val and test datasets\n",
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens.p\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "## 1. tokenization schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### default scheme vs. removing stopwords, stemming and lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, rm_stopwords = False, lemmatize = False, stem = False):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    if rm_stopwords:\n",
    "        stop_words = set(stopwords.words('english')) \n",
    "        all_tokens = [w for w in all_tokens if not w in stop_words] \n",
    "    if lemmatize:\n",
    "        lmtzr = WordNetLemmatizer()\n",
    "        all_tokens = [lmtzr.lemmatize(w) for w in all_tokens]\n",
    "    if stem:\n",
    "        ps = PorterStemmer()\n",
    "        all_tokens = [ps.stem(w) for w in all_tokens]\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "token2id_rmsw, id2token_rmsw = build_vocab(all_train_tokens, rm_stopwords = True)\n",
    "token2id_prcd, id2token_prcd = build_vocab(all_train_tokens, rm_stopwords = True, lemmatize = True, stem = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 2239 ; token asking\n",
      "Token asking; token id 2239\n",
      "Token asking; token id 8879\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[\"meadows\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 19923\n",
      "Val dataset size is 4981\n",
      "Test dataset size is 24678\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data, token2id_map = token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id_map[token] if token in token2id_map else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "train_data_indices_rmsw = token2index_dataset(train_data_tokens, token2id_map = token2id_rmsw)\n",
    "val_data_indices_rmsw = token2index_dataset(val_data_tokens, token2id_map = token2id_rmsw)\n",
    "\n",
    "train_data_indices_prcd = token2index_dataset(train_data_tokens, token2id_map = token2id_prcd)\n",
    "val_data_indices_prcd = token2index_dataset(val_data_tokens, token2id_map = token2id_prcd)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def data_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "train_dataset_rmsw = NewsGroupDataset(train_data_indices_rmsw, train_label)\n",
    "train_loader_rmsw = torch.utils.data.DataLoader(dataset=train_dataset_rmsw, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_rmsw = NewsGroupDataset(val_data_indices_rmsw, val_label)\n",
    "val_loader_rmsw = torch.utils.data.DataLoader(dataset=val_dataset_rmsw, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "train_dataset_prcd = NewsGroupDataset(train_data_indices_prcd, train_label)\n",
    "train_loader_prcd = torch.utils.data.DataLoader(dataset=train_dataset_prcd, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_prcd = NewsGroupDataset(val_data_indices_prcd, val_label)\n",
    "val_loader_prcd = torch.utils.data.DataLoader(dataset=val_dataset_prcd, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = []\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "        test_loss.append(loss.data.numpy())\n",
    "    #print(test_loss)\n",
    "    return (100 * correct / total), np.mean(test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default tokenization model training & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/623], Train Loss: 0.7854213118553162, Val Loss: 2.4459967613220215, Val Acc: 74.98494278257378\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.6038910746574402, Val Loss: 2.319542169570923, Val Acc: 84.01927323830556\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.5236561298370361, Val Loss: 2.2879624366760254, Val Acc: 86.10720738807468\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.47631028294563293, Val Loss: 2.273961067199707, Val Acc: 85.98674964866493\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.44805431365966797, Val Loss: 2.2669484615325928, Val Acc: 87.25155591246738\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.42757147550582886, Val Loss: 2.2585885524749756, Val Acc: 87.33186107207388\n",
      "Epoch: [1/10], Train Acc: 90.05169904130904\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.20235832035541534, Val Loss: 2.246875762939453, Val Acc: 87.45231881148364\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.2183261662721634, Val Loss: 2.2417376041412354, Val Acc: 87.39208994177876\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.2217005044221878, Val Loss: 2.238917112350464, Val Acc: 87.39208994177876\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.22531892359256744, Val Loss: 2.2404932975769043, Val Acc: 87.21140333266412\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.22787410020828247, Val Loss: 2.26181697845459, Val Acc: 83.99919694840393\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.22882162034511566, Val Loss: 2.2458784580230713, Val Acc: 85.84621561935354\n",
      "Epoch: [2/10], Train Acc: 94.81001857150028\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.15744182467460632, Val Loss: 2.2337353229522705, Val Acc: 86.70949608512348\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.15342436730861664, Val Loss: 2.2390191555023193, Val Acc: 86.1875125476812\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.16055384278297424, Val Loss: 2.23343825340271, Val Acc: 86.7295723750251\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.16291150450706482, Val Loss: 2.234325408935547, Val Acc: 86.46858060630396\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.16838929057121277, Val Loss: 2.2306718826293945, Val Acc: 87.05079301345111\n",
      "Epoch: [3/10], Step: [601/623], Train Loss: 0.17492902278900146, Val Loss: 2.239396810531616, Val Acc: 86.12728367797631\n",
      "Training stopped at epoch 3, iteration 601\n",
      "Default tokenization - Best Val Acc: 87.45231881148364\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_df = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_df.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Default tokenization - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only removing stopwords - model training & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/623], Train Loss: 0.7020294666290283, Val Loss: 2.4620540142059326, Val Acc: 65.06725557117045\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.5776280760765076, Val Loss: 2.3319802284240723, Val Acc: 83.55751857056816\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.5048703551292419, Val Loss: 2.289166212081909, Val Acc: 85.38446095161615\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.4653456509113312, Val Loss: 2.2735729217529297, Val Acc: 86.8901826942381\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.43591198325157166, Val Loss: 2.264702320098877, Val Acc: 86.9704878538446\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.41777071356773376, Val Loss: 2.26481032371521, Val Acc: 86.78980124472997\n",
      "Epoch: [1/10], Train Acc: 91.10575716508558\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.23542645573616028, Val Loss: 2.2481768131256104, Val Acc: 87.79361573981129\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.22903241217136383, Val Loss: 2.2442705631256104, Val Acc: 87.59285284079502\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.2272018939256668, Val Loss: 2.24578595161438, Val Acc: 87.09094559325436\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.23093031346797943, Val Loss: 2.254486560821533, Val Acc: 85.50491869102589\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.23483234643936157, Val Loss: 2.2456226348876953, Val Acc: 87.29170849227063\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.2354189157485962, Val Loss: 2.243398666381836, Val Acc: 86.70949608512348\n",
      "Epoch: [2/10], Train Acc: 93.344375847011\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.16104929149150848, Val Loss: 2.2419917583465576, Val Acc: 86.14735996787793\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.15865184366703033, Val Loss: 2.2376041412353516, Val Acc: 86.4284280265007\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.16163359582424164, Val Loss: 2.2338554859161377, Val Acc: 86.44850431640234\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.16778473556041718, Val Loss: 2.2320713996887207, Val Acc: 86.70949608512348\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.17518076300621033, Val Loss: 2.242164134979248, Val Acc: 85.5852238506324\n",
      "Epoch: [3/10], Step: [601/623], Train Loss: 0.18146665394306183, Val Loss: 2.2430615425109863, Val Acc: 85.70568159004216\n",
      "Training stopped at epoch 3, iteration 601\n",
      "Tokenization: only removing stopwords - Best Val Acc: 87.79361573981129\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token_rmsw), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_rmsw = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader_rmsw):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader_rmsw, model)\n",
    "            val_acc_rmsw.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader_rmsw), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader_rmsw, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Tokenization: only removing stopwords - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords, lemmetization and stemming - model training & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/623], Train Loss: 0.7367023229598999, Val Loss: 2.5087432861328125, Val Acc: 71.0098373820518\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.6420632004737854, Val Loss: 2.419785737991333, Val Acc: 71.43143946998595\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.5840003490447998, Val Loss: 2.363603353500366, Val Acc: 78.37783577594861\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.5463563799858093, Val Loss: 2.3428192138671875, Val Acc: 81.73057618952018\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.5200263261795044, Val Loss: 2.3371925354003906, Val Acc: 80.96767717325838\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.5034928321838379, Val Loss: 2.3362293243408203, Val Acc: 80.70668540453724\n",
      "Epoch: [1/10], Train Acc: 86.72890628921347\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.32985323667526245, Val Loss: 2.3236608505249023, Val Acc: 79.48203172053805\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.33238914608955383, Val Loss: 2.310842990875244, Val Acc: 81.18851636217627\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.3423231840133667, Val Loss: 2.3090784549713135, Val Acc: 82.43324633607709\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.347785085439682, Val Loss: 2.3057942390441895, Val Acc: 83.19614535233889\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.3520529866218567, Val Loss: 2.299783706665039, Val Acc: 83.39690825135514\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.3535850942134857, Val Loss: 2.302128314971924, Val Acc: 82.93515358361775\n",
      "Epoch: [2/10], Train Acc: 89.29880038146865\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.2912174463272095, Val Loss: 2.2951152324676514, Val Acc: 83.2764505119454\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.28586339950561523, Val Loss: 2.291790723800659, Val Acc: 82.5135514956836\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.29041531682014465, Val Loss: 2.293548583984375, Val Acc: 82.8950010038145\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.2927111089229584, Val Loss: 2.292647123336792, Val Acc: 82.95522987351937\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.29649826884269714, Val Loss: 2.292729616165161, Val Acc: 82.87492471391288\n",
      "Epoch: [3/10], Step: [601/623], Train Loss: 0.30029210448265076, Val Loss: 2.2940990924835205, Val Acc: 82.79461955430637\n",
      "Epoch: [3/10], Train Acc: 90.03664106811223\n",
      "Epoch: [4/10], Step: [101/623], Train Loss: 0.25151509046554565, Val Loss: 2.296450614929199, Val Acc: 81.54988958040555\n",
      "Epoch: [4/10], Step: [201/623], Train Loss: 0.26514098048210144, Val Loss: 2.2964203357696533, Val Acc: 81.26882152178277\n",
      "Epoch: [4/10], Step: [301/623], Train Loss: 0.27977725863456726, Val Loss: 2.292088031768799, Val Acc: 81.99156795824132\n",
      "Epoch: [4/10], Step: [401/623], Train Loss: 0.28101202845573425, Val Loss: 2.288591146469116, Val Acc: 82.23248343706084\n",
      "Training stopped at epoch 4, iteration 401\n",
      "Tokenization: removing stopwords, lemmetization and stemming - Best Val Acc: 83.39690825135514\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token_prcd), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_prcd = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader_prcd):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader_prcd, model)\n",
    "            val_acc_prcd.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader_prcd, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Tokenization: removing stopwords, lemmetization and stemming - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plotting training curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(val_acc_df)\n",
    "plt.plot(val_acc_rmsw)\n",
    "plt.plot(val_acc_prcd)\n",
    "plt.title('Tokenization schemes - Validation Acc')\n",
    "plt.legend(['Default', 'Removing stopwords', 'Removing stopwords, Lemmetization and Stemming'], loc='lower right')\n",
    "#plt.show()\n",
    "plt.savefig('Assignment_1/training_curve_tokenization.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It appears that using the default scheme performs almost the same with removing stopwords. Lemmetization and stemming seems an overkill for this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From now on, removing stopwords will be applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. N-gram (1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/623], Train Loss: 0.7620635032653809, Val Loss: 2.370851755142212, Val Acc: 80.08432041758682\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.5616288185119629, Val Loss: 2.279052972793579, Val Acc: 86.36819915679582\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.48318198323249817, Val Loss: 2.2621326446533203, Val Acc: 86.60911463561534\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.43653804063796997, Val Loss: 2.246751546859741, Val Acc: 88.35575185705682\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.4113127589225769, Val Loss: 2.2486064434051514, Val Acc: 87.53262397109015\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.39562875032424927, Val Loss: 2.256150960922241, Val Acc: 86.9704878538446\n",
      "Epoch: [1/10], Train Acc: 94.09225518245245\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.17512089014053345, Val Loss: 2.2260990142822266, Val Acc: 88.69704878538447\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.17568831145763397, Val Loss: 2.224189043045044, Val Acc: 88.63681991567958\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.1793377697467804, Val Loss: 2.2264115810394287, Val Acc: 87.77353944990966\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.18566608428955078, Val Loss: 2.2312204837799072, Val Acc: 87.39208994177876\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.19002342224121094, Val Loss: 2.225013256072998, Val Acc: 88.15498895804055\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.19690099358558655, Val Loss: 2.2275166511535645, Val Acc: 88.21521782774543\n",
      "Epoch: [2/10], Train Acc: 95.7636902072981\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.11009848862886429, Val Loss: 2.2229511737823486, Val Acc: 87.61292913069664\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.10979533195495605, Val Loss: 2.2149009704589844, Val Acc: 88.2754466974503\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.111215740442276, Val Loss: 2.21638560295105, Val Acc: 87.75346316000802\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.1234072744846344, Val Loss: 2.2175114154815674, Val Acc: 87.49247139128688\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.13115407526493073, Val Loss: 2.222806215286255, Val Acc: 87.29170849227063\n",
      "Epoch: [3/10], Step: [601/623], Train Loss: 0.1406969577074051, Val Loss: 2.2226319313049316, Val Acc: 87.43224252158201\n",
      "Training stopped at epoch 3, iteration 601\n",
      "Unigram - Best Val Acc: 88.69704878538447\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def tokenize_dataset(dataset, n = 1):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        stop_words = set(stopwords.words('english')) \n",
    "        tokens = [w for w in tokens if not w in stop_words] \n",
    "        if n > 1:\n",
    "            ngrams_gn = ngrams(tokens, n)\n",
    "            tokens = []\n",
    "            for gram in ngrams_gn:\n",
    "                tokens.append(gram)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "# # val set tokens\n",
    "# print (\"Tokenizing val data\")\n",
    "# val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "# pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# # train set tokens\n",
    "# print (\"Tokenizing train data\")\n",
    "# train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "# pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))\n",
    "\n",
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "#     if rm_stopwords:\n",
    "#         stop_words = set(stopwords.words('english')) \n",
    "#         all_tokens = [w for w in all_tokens if not w in stop_words] \n",
    "#     if lemmatize:\n",
    "#         lmtzr = WordNetLemmatizer()\n",
    "#         all_tokens = [lmtzr.lemmatize(w) for w in all_tokens]\n",
    "#     if stem:\n",
    "#         ps = PorterStemmer()\n",
    "#         all_tokens = [ps.stem(w) for w in all_tokens]\n",
    "#     if n > 1:\n",
    "#         all_tokens = ngrams(all_tokens, n)\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens, token2id_map = token2id)\n",
    "val_data_indices = token2index_dataset(val_data_tokens, token2id_map = token2id)\n",
    "\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_n1 = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_n1.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Unigram - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n",
    "\n",
    "del train_data_tokens, all_train_tokens, val_data_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "Tokenizing train data\n",
      "Epoch: [1/10], Step: [101/623], Train Loss: 0.7483088374137878, Val Loss: 2.539362668991089, Val Acc: 71.79281268821522\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.699461817741394, Val Loss: 2.473186492919922, Val Acc: 70.50793013451114\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.6441764831542969, Val Loss: 2.420319080352783, Val Acc: 72.33487251555913\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.5996631979942322, Val Loss: 2.3790340423583984, Val Acc: 77.59486046978519\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.5659245252609253, Val Loss: 2.3492419719696045, Val Acc: 80.98775346316\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.5415062308311462, Val Loss: 2.3542051315307617, Val Acc: 77.11302951214616\n",
      "Epoch: [1/10], Train Acc: 89.00767956633037\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.3272755444049835, Val Loss: 2.3255040645599365, Val Acc: 80.827143143947\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.3125482201576233, Val Loss: 2.31084942817688, Val Acc: 82.5135514956836\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.3089844882488251, Val Loss: 2.3107855319976807, Val Acc: 81.40935555109415\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.3068180978298187, Val Loss: 2.3004908561706543, Val Acc: 82.21240714715921\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.3034331798553467, Val Loss: 2.3005950450897217, Val Acc: 81.85103392892994\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.3071499168872833, Val Loss: 2.300405263900757, Val Acc: 82.23248343706084\n",
      "Epoch: [2/10], Train Acc: 91.89379109571851\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.20828741788864136, Val Loss: 2.2993366718292236, Val Acc: 81.12828749247139\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.20712073147296906, Val Loss: 2.2927393913269043, Val Acc: 81.5900421602088\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.21668988466262817, Val Loss: 2.315162181854248, Val Acc: 78.1369202971291\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.22146545350551605, Val Loss: 2.302515983581543, Val Acc: 79.20096366191528\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.2260867953300476, Val Loss: 2.289952516555786, Val Acc: 81.42943184099579\n",
      "Epoch: [3/10], Step: [601/623], Train Loss: 0.2305915802717209, Val Loss: 2.291659116744995, Val Acc: 81.12828749247139\n",
      "Epoch: [3/10], Train Acc: 91.18606635546855\n",
      "Epoch: [4/10], Step: [101/623], Train Loss: 0.14943791925907135, Val Loss: 2.286653518676758, Val Acc: 81.30897410158603\n",
      "Training stopped at epoch 4, iteration 101\n",
      "Bigram - Best Val Acc: 82.5135514956836\n"
     ]
    }
   ],
   "source": [
    "def tokenize_dataset(dataset, n = 1):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        stop_words = set(stopwords.words('english')) \n",
    "        tokens = [w for w in tokens if not w in stop_words] \n",
    "        if n > 1:\n",
    "            ngrams_gn = ngrams(tokens, n)\n",
    "            tokens = []\n",
    "            for gram in ngrams_gn:\n",
    "                tokens.append(gram)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "# val_data_tokens_n2, _ = tokenize_dataset(val_data, n = 2)\n",
    "# pkl.dump(val_data_tokens_n2, open(\"val_data_tokens_n2.p\", \"wb\"))\n",
    "\n",
    "# # train set tokens\n",
    "# print (\"Tokenizing train data\")\n",
    "# train_data_tokens_n2, all_train_tokens_n2 = tokenize_dataset(train_data, n = 2)\n",
    "# pkl.dump(train_data_tokens_n2, open(\"train_data_tokens_n2.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens_n2, open(\"all_train_tokens_n2.p\", \"wb\"))\n",
    "\n",
    "train_data_tokens_n2 = pkl.load(open(\"train_data_tokens_n2.p\", \"rb\"))\n",
    "all_train_tokens_n2 = pkl.load(open(\"all_train_tokens_n2.p\", \"rb\"))\n",
    "val_data_tokens_n2 = pkl.load(open(\"val_data_tokens_n2.p\", \"rb\"))\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "token2id_n2, id2token_n2 = build_vocab(all_train_tokens_n2)\n",
    "\n",
    "train_data_indices_n2 = token2index_dataset(train_data_tokens_n2, token2id_map = token2id_n2)\n",
    "val_data_indices_n2 = token2index_dataset(val_data_tokens_n2, token2id_map = token2id_n2)\n",
    "\n",
    "train_dataset = NewsGroupDataset(train_data_indices_n2, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices_n2, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token_n2), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_n2 = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_n2.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Bigram - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n",
    "\n",
    "del train_data_tokens_n2, all_train_tokens_n2, val_data_tokens_n2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "Tokenizing train data\n",
      "Epoch: [1/10], Step: [101/623], Train Loss: 0.762969970703125, Val Loss: 2.557878017425537, Val Acc: 49.7691226661313\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.7300273776054382, Val Loss: 2.554076671600342, Val Acc: 51.897209395703676\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.7226154208183289, Val Loss: 2.547767400741577, Val Acc: 61.413370809074486\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.7110732793807983, Val Loss: 2.5318849086761475, Val Acc: 56.91628187111022\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.6981146335601807, Val Loss: 2.524343252182007, Val Acc: 52.35896406344108\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.6842151284217834, Val Loss: 2.5000407695770264, Val Acc: 59.68680987753463\n",
      "Epoch: [1/10], Train Acc: 75.10415098127793\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.5393630862236023, Val Loss: 2.472050905227661, Val Acc: 69.08251355149568\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.5244731903076172, Val Loss: 2.4593935012817383, Val Acc: 69.62457337883959\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.515049934387207, Val Loss: 2.448827028274536, Val Acc: 68.80144549287292\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.5091409087181091, Val Loss: 2.444082021713257, Val Acc: 66.85404537241517\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.5114555954933167, Val Loss: 2.439819574356079, Val Acc: 70.72876932342903\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.5126630067825317, Val Loss: 2.4417643547058105, Val Acc: 67.09496085123469\n",
      "Epoch: [2/10], Train Acc: 71.75124228278874\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.43528369069099426, Val Loss: 2.4331116676330566, Val Acc: 68.05862276651276\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.42108696699142456, Val Loss: 2.4270145893096924, Val Acc: 69.64464966874122\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.41931483149528503, Val Loss: 2.4244184494018555, Val Acc: 70.38747239510138\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.42138010263442993, Val Loss: 2.4280574321746826, Val Acc: 66.37221441477615\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.4225616455078125, Val Loss: 2.4274256229400635, Val Acc: 66.4123669945794\n",
      "Epoch: [3/10], Step: [601/623], Train Loss: 0.42505720257759094, Val Loss: 2.4212231636047363, Val Acc: 70.18670949608513\n",
      "Epoch: [3/10], Train Acc: 81.33815188475631\n",
      "Epoch: [4/10], Step: [101/623], Train Loss: 0.3624342083930969, Val Loss: 2.417597532272339, Val Acc: 69.3234290303152\n",
      "Epoch: [4/10], Step: [201/623], Train Loss: 0.3652028739452362, Val Loss: 2.4190099239349365, Val Acc: 66.39229070467778\n",
      "Epoch: [4/10], Step: [301/623], Train Loss: 0.36948689818382263, Val Loss: 2.4180057048797607, Val Acc: 66.59305360369403\n",
      "Epoch: [4/10], Step: [401/623], Train Loss: 0.37124109268188477, Val Loss: 2.4149398803710938, Val Acc: 69.50411563942983\n",
      "Training stopped at epoch 4, iteration 401\n",
      "Trigram - Best Val Acc: 70.72876932342903\n"
     ]
    }
   ],
   "source": [
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens_n3, _ = tokenize_dataset(val_data, n = 3)\n",
    "pkl.dump(val_data_tokens_n3, open(\"val_data_tokens_n3.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens_n3, all_train_tokens_n3 = tokenize_dataset(train_data, n = 3)\n",
    "pkl.dump(train_data_tokens_n3, open(\"train_data_tokens_n3.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_n3, open(\"all_train_tokens_n3.p\", \"wb\"))\n",
    "\n",
    "train_data_tokens_n3 = pkl.load(open(\"train_data_tokens_n3.p\", \"rb\"))\n",
    "all_train_tokens_n3 = pkl.load(open(\"all_train_tokens_n3.p\", \"rb\"))\n",
    "val_data_tokens_n3 = pkl.load(open(\"val_data_tokens_n3.p\", \"rb\"))\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "token2id_n3, id2token_n3 = build_vocab(all_train_tokens_n3)\n",
    "\n",
    "train_data_indices_n3 = token2index_dataset(train_data_tokens_n3, token2id_map = token2id_n3)\n",
    "val_data_indices_n3 = token2index_dataset(val_data_tokens_n3, token2id_map = token2id_n3)\n",
    "\n",
    "train_dataset = NewsGroupDataset(train_data_indices_n3, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices_n3, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token_n3), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_n3 = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_n3.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Trigram - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n",
    "\n",
    "del train_data_tokens_n3, all_train_tokens_n3, val_data_tokens_n3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Four-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "Tokenizing train data\n",
      "Epoch: [1/10], Step: [101/623], Train Loss: 0.7611814737319946, Val Loss: 2.558610677719116, Val Acc: 49.74904637622967\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.7389953136444092, Val Loss: 2.5604302883148193, Val Acc: 50.2308773338687\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.7323024272918701, Val Loss: 2.5578179359436035, Val Acc: 50.29110620357358\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.72617506980896, Val Loss: 2.557344913482666, Val Acc: 50.13049588436057\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.7236428260803223, Val Loss: 2.5577173233032227, Val Acc: 50.351335073278456\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.720410943031311, Val Loss: 2.559602737426758, Val Acc: 49.7691226661313\n",
      "Epoch: [1/10], Train Acc: 50.72529237564624\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.6859458684921265, Val Loss: 2.553286075592041, Val Acc: 51.73659907649066\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.6889374852180481, Val Loss: 2.551931142807007, Val Acc: 50.65247942180285\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.6889548301696777, Val Loss: 2.5491340160369873, Val Acc: 52.07789600481831\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.6872836351394653, Val Loss: 2.5460920333862305, Val Acc: 53.041557920096366\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.6840561032295227, Val Loss: 2.5421178340911865, Val Acc: 55.43063641838988\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.6796876788139343, Val Loss: 2.5381600856781006, Val Acc: 55.43063641838988\n",
      "Epoch: [2/10], Train Acc: 55.86006123575767\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.6363542675971985, Val Loss: 2.535537004470825, Val Acc: 53.84460951616141\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.6222201585769653, Val Loss: 2.53283429145813, Val Acc: 53.46316000803051\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.6183652877807617, Val Loss: 2.5326988697052, Val Acc: 53.041557920096366\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.6144606471061707, Val Loss: 2.5411810874938965, Val Acc: 51.55591246737603\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.6166439652442932, Val Loss: 2.5256359577178955, Val Acc: 55.10941577996386\n",
      "Epoch: [3/10], Step: [601/623], Train Loss: 0.6133341193199158, Val Loss: 2.522416830062866, Val Acc: 55.61132302750452\n",
      "Epoch: [3/10], Train Acc: 66.53616423229434\n",
      "Epoch: [4/10], Step: [101/623], Train Loss: 0.5552462339401245, Val Loss: 2.520672559738159, Val Acc: 55.189720939570364\n",
      "Epoch: [4/10], Step: [201/623], Train Loss: 0.5598178505897522, Val Loss: 2.515700101852417, Val Acc: 55.87231479622566\n",
      "Epoch: [4/10], Step: [301/623], Train Loss: 0.5573968887329102, Val Loss: 2.5127644538879395, Val Acc: 58.140935555109415\n",
      "Epoch: [4/10], Step: [401/623], Train Loss: 0.5568462610244751, Val Loss: 2.51163649559021, Val Acc: 58.24131700461755\n",
      "Epoch: [4/10], Step: [501/623], Train Loss: 0.5602888464927673, Val Loss: 2.5114152431488037, Val Acc: 58.18108813491267\n",
      "Epoch: [4/10], Step: [601/623], Train Loss: 0.5577936172485352, Val Loss: 2.5094268321990967, Val Acc: 58.04055410560129\n",
      "Epoch: [4/10], Train Acc: 68.79987953621442\n",
      "Epoch: [5/10], Step: [101/623], Train Loss: 0.526072084903717, Val Loss: 2.5125253200531006, Val Acc: 56.675366392290705\n",
      "Epoch: [5/10], Step: [201/623], Train Loss: 0.525156557559967, Val Loss: 2.507481813430786, Val Acc: 57.71933346717527\n",
      "Epoch: [5/10], Step: [301/623], Train Loss: 0.5315836668014526, Val Loss: 2.508286714553833, Val Acc: 57.438265408552496\n",
      "Epoch: [5/10], Step: [401/623], Train Loss: 0.5294779539108276, Val Loss: 2.50811505317688, Val Acc: 57.27765508933949\n",
      "Epoch: [5/10], Step: [501/623], Train Loss: 0.5326103568077087, Val Loss: 2.5111634731292725, Val Acc: 55.73178076691428\n",
      "Epoch: [5/10], Step: [601/623], Train Loss: 0.5299821496009827, Val Loss: 2.5053470134735107, Val Acc: 57.9000200762899\n",
      "Epoch: [5/10], Train Acc: 67.69562816844852\n",
      "Epoch: [6/10], Step: [101/623], Train Loss: 0.49497437477111816, Val Loss: 2.50458025932312, Val Acc: 58.080706685404536\n",
      "Epoch: [6/10], Step: [201/623], Train Loss: 0.5066328048706055, Val Loss: 2.5097694396972656, Val Acc: 55.89239108612728\n",
      "Epoch: [6/10], Step: [301/623], Train Loss: 0.4997906982898712, Val Loss: 2.5055363178253174, Val Acc: 55.81208592652078\n",
      "Training stopped at epoch 6, iteration 301\n",
      "Fourgram - Best Val Acc: 58.24131700461755\n"
     ]
    }
   ],
   "source": [
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens_n4, _ = tokenize_dataset(val_data, n = 4)\n",
    "pkl.dump(val_data_tokens_n4, open(\"val_data_tokens_n4.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens_n4, all_train_tokens_n4 = tokenize_dataset(train_data, n = 4)\n",
    "pkl.dump(train_data_tokens_n4, open(\"train_data_tokens_n4.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_n4, open(\"all_train_tokens_n4.p\", \"wb\"))\n",
    "\n",
    "train_data_tokens_n4 = pkl.load(open(\"train_data_tokens_n4.p\", \"rb\"))\n",
    "all_train_tokens_n4 = pkl.load(open(\"all_train_tokens_n4.p\", \"rb\"))\n",
    "val_data_tokens_n4 = pkl.load(open(\"val_data_tokens_n4.p\", \"rb\"))\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "token2id_n4, id2token_n4 = build_vocab(all_train_tokens_n4)\n",
    "\n",
    "train_data_indices_n4 = token2index_dataset(train_data_tokens_n4, token2id_map = token2id_n4)\n",
    "val_data_indices_n4 = token2index_dataset(val_data_tokens_n4, token2id_map = token2id_n4)\n",
    "\n",
    "train_dataset = NewsGroupDataset(train_data_indices_n4, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices_n4, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token_n4), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_n4 = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_n4.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Fourgram - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n",
    "\n",
    "del train_data_tokens_n4, all_train_tokens_n4, val_data_tokens_n4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_acc_n1)\n",
    "plt.plot(val_acc_n2)\n",
    "plt.plot(val_acc_n3)\n",
    "plt.plot(val_acc_n4)\n",
    "plt.title('N-gram schemes - Validation Acc')\n",
    "plt.legend(['Unigram', 'Bigram', 'Trigram', 'Fourgram'], loc='upper right')\n",
    "#plt.show()\n",
    "plt.savefig('Assignment_1/training_curve_ngrams.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram outperforms 2-4 N-grams probably due to the nature of the binary classification task where positive words are likely linked to positive review. Using unigram from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vocabulary size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/623], Train Loss: 0.7321062088012695, Val Loss: 2.3889167308807373, Val Acc: 81.5900421602088\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.56125408411026, Val Loss: 2.296505928039551, Val Acc: 85.54507127082915\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.48898589611053467, Val Loss: 2.2715840339660645, Val Acc: 86.66934350532021\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.44337064027786255, Val Loss: 2.2644059658050537, Val Acc: 86.68941979522184\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.4157082736492157, Val Loss: 2.2547972202301025, Val Acc: 87.1913270427625\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.3992776870727539, Val Loss: 2.2593066692352295, Val Acc: 86.78980124472997\n",
      "Epoch: [1/10], Train Acc: 91.53741906339407\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.223252072930336, Val Loss: 2.240558624267578, Val Acc: 87.63300542059828\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.22885841131210327, Val Loss: 2.2367727756500244, Val Acc: 87.77353944990966\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.23231744766235352, Val Loss: 2.2397847175598145, Val Acc: 87.09094559325436\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.22951233386993408, Val Loss: 2.2342283725738525, Val Acc: 87.45231881148364\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.2349730134010315, Val Loss: 2.2388670444488525, Val Acc: 87.25155591246738\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.23960451781749725, Val Loss: 2.2437682151794434, Val Acc: 87.07086930335274\n",
      "Epoch: [2/10], Train Acc: 92.1849119108568\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.18287847936153412, Val Loss: 2.2313873767852783, Val Acc: 87.49247139128688\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.18769843876361847, Val Loss: 2.233045816421509, Val Acc: 86.78980124472997\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.19035236537456512, Val Loss: 2.2302680015563965, Val Acc: 87.33186107207388\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.19467580318450928, Val Loss: 2.2313849925994873, Val Acc: 86.82995382453322\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.2061786949634552, Val Loss: 2.2365036010742188, Val Acc: 86.95041156394298\n",
      "Epoch: [3/10], Step: [601/623], Train Loss: 0.20951972901821136, Val Loss: 2.234450578689575, Val Acc: 86.93033527404135\n",
      "Epoch: [3/10], Train Acc: 93.499974903378\n",
      "Epoch: [4/10], Step: [101/623], Train Loss: 0.16008137166500092, Val Loss: 2.232013702392578, Val Acc: 86.74964866492672\n",
      "Training stopped at epoch 4, iteration 101\n",
      "Vocabulary size: 5000 - Best Val Acc: 87.77353944990966\n"
     ]
    }
   ],
   "source": [
    "def tokenize_dataset(dataset, n = 1):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        stop_words = set(stopwords.words('english')) \n",
    "        tokens = [w for w in tokens if not w in stop_words] \n",
    "        if n > 1:\n",
    "            ngrams_gn = ngrams(tokens, n)\n",
    "            tokens = []\n",
    "            for gram in ngrams_gn:\n",
    "                tokens.append(gram)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "# # val set tokens\n",
    "# print (\"Tokenizing val data\")\n",
    "# val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "# pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# # train set tokens\n",
    "# print (\"Tokenizing train data\")\n",
    "# train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "# pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))\n",
    "\n",
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size = 10000):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "#     if rm_stopwords:\n",
    "#         stop_words = set(stopwords.words('english')) \n",
    "#         all_tokens = [w for w in all_tokens if not w in stop_words] \n",
    "#     if lemmatize:\n",
    "#         lmtzr = WordNetLemmatizer()\n",
    "#         all_tokens = [lmtzr.lemmatize(w) for w in all_tokens]\n",
    "#     if stem:\n",
    "#         ps = PorterStemmer()\n",
    "#         all_tokens = [ps.stem(w) for w in all_tokens]\n",
    "#     if n > 1:\n",
    "#         all_tokens = ngrams(all_tokens, n)\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = 5000)\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens, token2id_map = token2id)\n",
    "val_data_indices = token2index_dataset(val_data_tokens, token2id_map = token2id)\n",
    "\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_5k = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_5k.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Vocabulary size: 5000 - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/623], Train Loss: 0.7739497423171997, Val Loss: 2.382624626159668, Val Acc: 81.30897410158603\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.5694124698638916, Val Loss: 2.278719663619995, Val Acc: 86.85003011443486\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.4913446307182312, Val Loss: 2.2656798362731934, Val Acc: 87.07086930335274\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.44541850686073303, Val Loss: 2.2531917095184326, Val Acc: 87.43224252158201\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.41131487488746643, Val Loss: 2.239922046661377, Val Acc: 88.75727765508934\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.393562912940979, Val Loss: 2.2424280643463135, Val Acc: 88.75727765508934\n",
      "Epoch: [1/10], Train Acc: 94.36831802439391\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.1839006096124649, Val Loss: 2.2298169136047363, Val Acc: 88.23529411764706\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.16838745772838593, Val Loss: 2.2188570499420166, Val Acc: 88.63681991567958\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.17500756680965424, Val Loss: 2.2235019207000732, Val Acc: 88.69704878538447\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.18115989863872528, Val Loss: 2.224781036376953, Val Acc: 88.43605701666333\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.18867367506027222, Val Loss: 2.228792905807495, Val Acc: 87.9542260590243\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.19484451413154602, Val Loss: 2.226625919342041, Val Acc: 88.25537040754868\n",
      "Epoch: [2/10], Train Acc: 96.8127290066757\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.10292168706655502, Val Loss: 2.2177369594573975, Val Acc: 88.21521782774543\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.12048788368701935, Val Loss: 2.2202470302581787, Val Acc: 87.91407347922105\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.12343841046094894, Val Loss: 2.219914674758911, Val Acc: 87.29170849227063\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.12597434222698212, Val Loss: 2.2191295623779297, Val Acc: 87.55270026099177\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.1285269409418106, Val Loss: 2.2199530601501465, Val Acc: 87.15117446295925\n",
      "Training stopped at epoch 3, iteration 501\n",
      "Vocabulary size: 10000 - Best Val Acc: 88.75727765508934\n"
     ]
    }
   ],
   "source": [
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = 10000)\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens, token2id_map = token2id)\n",
    "val_data_indices = token2index_dataset(val_data_tokens, token2id_map = token2id)\n",
    "\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_10k = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_10k.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Vocabulary size: 10000 - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/623], Train Loss: 0.8178815841674805, Val Loss: 2.3857476711273193, Val Acc: 81.14836378237301\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.5866197943687439, Val Loss: 2.2788734436035156, Val Acc: 85.76591045974703\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.5026383399963379, Val Loss: 2.2585244178771973, Val Acc: 88.55651475607307\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.45770734548568726, Val Loss: 2.2469608783721924, Val Acc: 89.29933748243324\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.42349937558174133, Val Loss: 2.2354540824890137, Val Acc: 89.25918490263\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.4017529785633087, Val Loss: 2.2363617420196533, Val Acc: 89.27926119253162\n",
      "Epoch: [1/10], Train Acc: 95.11117803543642\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.14968909323215485, Val Loss: 2.218065023422241, Val Acc: 89.60048183095763\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.14594332873821259, Val Loss: 2.2186381816864014, Val Acc: 89.19895603292511\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.14714498817920685, Val Loss: 2.213834762573242, Val Acc: 89.35956635213813\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.14723625779151917, Val Loss: 2.217393636703491, Val Acc: 88.6568962055812\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.1532984972000122, Val Loss: 2.2159581184387207, Val Acc: 89.15880345312186\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.15746967494487762, Val Loss: 2.218958616256714, Val Acc: 88.81750652479421\n",
      "Epoch: [2/10], Train Acc: 98.19304321638307\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.07077063620090485, Val Loss: 2.214143991470337, Val Acc: 88.81750652479421\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.06696520000696182, Val Loss: 2.209084987640381, Val Acc: 88.75727765508934\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.06804017722606659, Val Loss: 2.210110902786255, Val Acc: 88.15498895804055\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.07198123633861542, Val Loss: 2.2113850116729736, Val Acc: 87.71331058020478\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.07665906101465225, Val Loss: 2.211921215057373, Val Acc: 87.67315800040153\n",
      "Epoch: [3/10], Step: [601/623], Train Loss: 0.08103351294994354, Val Loss: 2.214082717895508, Val Acc: 87.59285284079502\n",
      "Training stopped at epoch 3, iteration 601\n",
      "Vocabulary size: 5000 - Best Val Acc: 89.60048183095763\n"
     ]
    }
   ],
   "source": [
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = 20000)\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens, token2id_map = token2id)\n",
    "val_data_indices = token2index_dataset(val_data_tokens, token2id_map = token2id)\n",
    "\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_20k = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_20k.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Vocabulary size: 5000 - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_acc_5k)\n",
    "plt.plot(val_acc_10k)\n",
    "plt.plot(val_acc_20k)\n",
    "plt.title('Vocabulary size tuning - Validation Acc')\n",
    "plt.legend(['5000', '10000', '20000'], loc='upper right')\n",
    "#plt.show()\n",
    "plt.savefig('Assignment_1/training_curve_vocabsize.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary size of 20K outperforms 10K by a small margin and 5K. Using 20K from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/623], Train Loss: 0.9647859334945679, Val Loss: 2.446251630783081, Val Acc: 78.77936157398113\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.6910555362701416, Val Loss: 2.3025712966918945, Val Acc: 86.22766512748444\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.5670194625854492, Val Loss: 2.26542067527771, Val Acc: 87.45231881148364\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.5064772963523865, Val Loss: 2.252769708633423, Val Acc: 88.05460750853243\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.4633772671222687, Val Loss: 2.241203784942627, Val Acc: 88.89781168440072\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.43192681670188904, Val Loss: 2.233466625213623, Val Acc: 89.52017667135114\n",
      "Epoch: [1/10], Train Acc: 94.92044370827686\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.1563727706670761, Val Loss: 2.2226510047912598, Val Acc: 89.58040554105601\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.15276536345481873, Val Loss: 2.2178306579589844, Val Acc: 89.11865087331861\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.15492577850818634, Val Loss: 2.2241270542144775, Val Acc: 88.35575185705682\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.16074320673942566, Val Loss: 2.217947244644165, Val Acc: 89.07849829351535\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.16533224284648895, Val Loss: 2.2194719314575195, Val Acc: 89.09857458341699\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.1674017608165741, Val Loss: 2.2185370922088623, Val Acc: 88.53643846617145\n",
      "Epoch: [2/10], Train Acc: 98.1528886211916\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.07367120683193207, Val Loss: 2.2164556980133057, Val Acc: 88.6568962055812\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.07379666715860367, Val Loss: 2.216675281524658, Val Acc: 87.87392089941778\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.07695472240447998, Val Loss: 2.2136070728302, Val Acc: 88.01445492872917\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.07973968982696533, Val Loss: 2.2131128311157227, Val Acc: 88.07468379843405\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.08456562459468842, Val Loss: 2.2122371196746826, Val Acc: 88.2754466974503\n",
      "Epoch: [3/10], Step: [601/623], Train Loss: 0.08684825152158737, Val Loss: 2.2128450870513916, Val Acc: 87.77353944990966\n",
      "Training stopped at epoch 3, iteration 601\n",
      "Embedding size: 50 - Best Val Acc: 89.58040554105601\n"
     ]
    }
   ],
   "source": [
    "# # val set tokens\n",
    "# print (\"Tokenizing val data\")\n",
    "# val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "# pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# # train set tokens\n",
    "# print (\"Tokenizing train data\")\n",
    "# train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "# pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))\n",
    "\n",
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size = 10000):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "#     if rm_stopwords:\n",
    "#         stop_words = set(stopwords.words('english')) \n",
    "#         all_tokens = [w for w in all_tokens if not w in stop_words] \n",
    "#     if lemmatize:\n",
    "#         lmtzr = WordNetLemmatizer()\n",
    "#         all_tokens = [lmtzr.lemmatize(w) for w in all_tokens]\n",
    "#     if stem:\n",
    "#         ps = PorterStemmer()\n",
    "#         all_tokens = [ps.stem(w) for w in all_tokens]\n",
    "#     if n > 1:\n",
    "#         all_tokens = ngrams(all_tokens, n)\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = 20000)\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens, token2id_map = token2id)\n",
    "val_data_indices = token2index_dataset(val_data_tokens, token2id_map = token2id)\n",
    "\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "emb_dim = 50\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_50 = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_50.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Embedding size: 50 - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/623], Train Loss: 0.8266407251358032, Val Loss: 2.3817405700683594, Val Acc: 83.85866291909255\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.5869506597518921, Val Loss: 2.271872043609619, Val Acc: 87.45231881148364\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.4989827871322632, Val Loss: 2.254636287689209, Val Acc: 88.05460750853243\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.4516891837120056, Val Loss: 2.2437167167663574, Val Acc: 88.95804055410561\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.41880297660827637, Val Loss: 2.238386631011963, Val Acc: 89.15880345312186\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.3984505534172058, Val Loss: 2.2399024963378906, Val Acc: 89.23910861272837\n",
      "Epoch: [1/10], Train Acc: 95.85905737087788\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.14840933680534363, Val Loss: 2.2228729724884033, Val Acc: 89.11865087331861\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.14452499151229858, Val Loss: 2.2160263061523438, Val Acc: 88.89781168440072\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.14752744138240814, Val Loss: 2.216421604156494, Val Acc: 88.93796426420397\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.15084585547447205, Val Loss: 2.218679428100586, Val Acc: 88.73720136518772\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.1514774113893509, Val Loss: 2.2173495292663574, Val Acc: 88.53643846617145\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.15883445739746094, Val Loss: 2.2218120098114014, Val Acc: 88.37582814695844\n",
      "Epoch: [2/10], Train Acc: 98.34864227275008\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.0731857642531395, Val Loss: 2.2123780250549316, Val Acc: 88.55651475607307\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.06839191913604736, Val Loss: 2.2098145484924316, Val Acc: 88.51636217626982\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.06899742037057877, Val Loss: 2.212519645690918, Val Acc: 87.87392089941778\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.07100990414619446, Val Loss: 2.2123587131500244, Val Acc: 87.93414976912267\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.07382798194885254, Val Loss: 2.211919069290161, Val Acc: 87.55270026099177\n",
      "Training stopped at epoch 3, iteration 501\n",
      "Embedding size: 100 - Best Val Acc: 89.23910861272837\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_100 = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_100.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Embedding size: 100 - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/623], Train Loss: 0.6745443940162659, Val Loss: 2.3205745220184326, Val Acc: 83.8988154988958\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.5088608264923096, Val Loss: 2.268181324005127, Val Acc: 86.93033527404135\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.44860270619392395, Val Loss: 2.252514600753784, Val Acc: 87.6530817104999\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.410873681306839, Val Loss: 2.2482879161834717, Val Acc: 87.6530817104999\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.38457000255584717, Val Loss: 2.2381114959716797, Val Acc: 88.29552298735194\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.37160301208496094, Val Loss: 2.2446820735931396, Val Acc: 87.93414976912267\n",
      "Epoch: [1/10], Train Acc: 95.50268533855343\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.1339179277420044, Val Loss: 2.216309070587158, Val Acc: 88.79743023489259\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.1401384323835373, Val Loss: 2.223104476928711, Val Acc: 87.97430234892592\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.14005914330482483, Val Loss: 2.221510171890259, Val Acc: 87.87392089941778\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.1478622406721115, Val Loss: 2.221844434738159, Val Acc: 88.19514153784381\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.15175694227218628, Val Loss: 2.225693464279175, Val Acc: 87.63300542059828\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.1556168496608734, Val Loss: 2.223449945449829, Val Acc: 87.81369202971291\n",
      "Epoch: [2/10], Train Acc: 98.38879686794158\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.06296572089195251, Val Loss: 2.21291184425354, Val Acc: 87.91407347922105\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.06384988874197006, Val Loss: 2.2117016315460205, Val Acc: 87.85384460951616\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.06619667261838913, Val Loss: 2.213991165161133, Val Acc: 87.53262397109015\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.06987205147743225, Val Loss: 2.210521936416626, Val Acc: 87.99437863882754\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.07434387505054474, Val Loss: 2.2138874530792236, Val Acc: 87.47239510138526\n",
      "Epoch: [3/10], Step: [601/623], Train Loss: 0.076358862221241, Val Loss: 2.2153773307800293, Val Acc: 87.1913270427625\n",
      "Training stopped at epoch 3, iteration 601\n",
      "Embedding size: 200 - Best Val Acc: 88.79743023489259\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 200\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_200 = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_200.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Embedding size: 200 - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_acc_50)\n",
    "plt.plot(val_acc_100)\n",
    "plt.plot(val_acc_200)\n",
    "plt.title('Embedding size tuning - Validation Acc')\n",
    "plt.legend(['50', '100', '200'], loc='upper right')\n",
    "#plt.show()\n",
    "plt.savefig('Assignment_1/training_curve_embedsize.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding size of 50 slightly outperforms 100 and 200. Although the performance of embedding size of 100 is slightly lower than 3.c. To be consistent, embedding size of 50 will be used from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sentence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/623], Train Loss: 1.0119531154632568, Val Loss: 2.464231491088867, Val Acc: 76.9925717727364\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.7320461273193359, Val Loss: 2.3289899826049805, Val Acc: 83.69805259987955\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.6097546219825745, Val Loss: 2.285020589828491, Val Acc: 85.78598674964867\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.5411535501480103, Val Loss: 2.270019054412842, Val Acc: 86.4284280265007\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.49606838822364807, Val Loss: 2.265040874481201, Val Acc: 86.87010640433648\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.46678975224494934, Val Loss: 2.2604024410247803, Val Acc: 87.17125075286087\n",
      "Epoch: [1/10], Train Acc: 94.11735180444713\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.18623584508895874, Val Loss: 2.2438957691192627, Val Acc: 87.45231881148364\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.18079297244548798, Val Loss: 2.2398881912231445, Val Acc: 87.47239510138526\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.18322055041790009, Val Loss: 2.238334894180298, Val Acc: 87.15117446295925\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.18915043771266937, Val Loss: 2.2408034801483154, Val Acc: 86.64926721541859\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.19298119843006134, Val Loss: 2.237220287322998, Val Acc: 87.23147962256574\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.19596070051193237, Val Loss: 2.236715793609619, Val Acc: 87.25155591246738\n",
      "Epoch: [2/10], Train Acc: 97.60076293730864\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.10670601576566696, Val Loss: 2.2335026264190674, Val Acc: 86.8901826942381\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.10233137756586075, Val Loss: 2.231062173843384, Val Acc: 86.66934350532021\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.10252758115530014, Val Loss: 2.2308733463287354, Val Acc: 86.28789399718931\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.10186979919672012, Val Loss: 2.2289788722991943, Val Acc: 86.4284280265007\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.10655767470598221, Val Loss: 2.2286465167999268, Val Acc: 86.20758883758282\n",
      "Epoch: [3/10], Step: [601/623], Train Loss: 0.10993137210607529, Val Loss: 2.2308239936828613, Val Acc: 86.02690222846817\n",
      "Epoch: [3/10], Train Acc: 98.97605782261708\n",
      "Epoch: [4/10], Step: [101/623], Train Loss: 0.044284217059612274, Val Loss: 2.2316768169403076, Val Acc: 85.86629190925517\n",
      "Training stopped at epoch 4, iteration 101\n",
      "Max sentence length: 100 - Best Val Acc: 87.47239510138526\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size = 10000):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "#     if rm_stopwords:\n",
    "#         stop_words = set(stopwords.words('english')) \n",
    "#         all_tokens = [w for w in all_tokens if not w in stop_words] \n",
    "#     if lemmatize:\n",
    "#         lmtzr = WordNetLemmatizer()\n",
    "#         all_tokens = [lmtzr.lemmatize(w) for w in all_tokens]\n",
    "#     if stem:\n",
    "#         ps = PorterStemmer()\n",
    "#         all_tokens = [ps.stem(w) for w in all_tokens]\n",
    "#     if n > 1:\n",
    "#         all_tokens = ngrams(all_tokens, n)\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = 20000)\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens, token2id_map = token2id)\n",
    "val_data_indices = token2index_dataset(val_data_tokens, token2id_map = token2id)\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 100\n",
    "\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "emb_dim = 50\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_sl100 = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_sl100.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Max sentence length: 100 - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/623], Train Loss: 0.9876643419265747, Val Loss: 2.452660083770752, Val Acc: 77.95623368801445\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.7087804675102234, Val Loss: 2.311063528060913, Val Acc: 85.9666733587633\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.5798584818840027, Val Loss: 2.2650294303894043, Val Acc: 87.83376831961453\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.512056291103363, Val Loss: 2.252634048461914, Val Acc: 88.45613330656495\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.46955904364585876, Val Loss: 2.246859312057495, Val Acc: 88.8777353944991\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.4383469223976135, Val Loss: 2.238823890686035, Val Acc: 89.3394900622365\n",
      "Epoch: [1/10], Train Acc: 94.88028911308538\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.15114255249500275, Val Loss: 2.2255375385284424, Val Acc: 88.69704878538447\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.14917483925819397, Val Loss: 2.219890832901001, Val Acc: 88.69704878538447\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.15360790491104126, Val Loss: 2.220153331756592, Val Acc: 88.69704878538447\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.15815912187099457, Val Loss: 2.2187581062316895, Val Acc: 89.09857458341699\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.16229495406150818, Val Loss: 2.2209556102752686, Val Acc: 88.43605701666333\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.16824358701705933, Val Loss: 2.221282720565796, Val Acc: 88.59666733587633\n",
      "Epoch: [2/10], Train Acc: 98.20810118957988\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.08091362565755844, Val Loss: 2.217968702316284, Val Acc: 88.25537040754868\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.07963627576828003, Val Loss: 2.212604284286499, Val Acc: 88.63681991567958\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.0794302299618721, Val Loss: 2.210625410079956, Val Acc: 88.47620959646657\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.08048326522111893, Val Loss: 2.2113749980926514, Val Acc: 88.23529411764706\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.08256632834672928, Val Loss: 2.2134323120117188, Val Acc: 87.83376831961453\n",
      "Training stopped at epoch 3, iteration 501\n",
      "Max sentence length: 200 - Best Val Acc: 89.3394900622365\n"
     ]
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "emb_dim = 50\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_sl200 = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_sl200.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Max sentence length: 200 - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/623], Train Loss: 0.974026083946228, Val Loss: 2.468712091445923, Val Acc: 79.54226059024292\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.6973989009857178, Val Loss: 2.307241201400757, Val Acc: 85.2037743425015\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.5751442909240723, Val Loss: 2.268345355987549, Val Acc: 87.47239510138526\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.5032266974449158, Val Loss: 2.246661424636841, Val Acc: 88.53643846617145\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.4583907723426819, Val Loss: 2.235736846923828, Val Acc: 89.3394900622365\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.42950743436813354, Val Loss: 2.2346317768096924, Val Acc: 89.19895603292511\n",
      "Epoch: [1/10], Train Acc: 94.51387843196306\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.1499275118112564, Val Loss: 2.222388982772827, Val Acc: 89.35956635213813\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.14635641872882843, Val Loss: 2.2159221172332764, Val Acc: 89.45994780164625\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.1476910412311554, Val Loss: 2.214937686920166, Val Acc: 89.31941377233487\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.15311937034130096, Val Loss: 2.2143497467041016, Val Acc: 89.29933748243324\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.15897685289382935, Val Loss: 2.2150983810424805, Val Acc: 88.95804055410561\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.16641667485237122, Val Loss: 2.2180802822113037, Val Acc: 88.95804055410561\n",
      "Epoch: [2/10], Train Acc: 97.76640064247353\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.06880000978708267, Val Loss: 2.2128703594207764, Val Acc: 89.19895603292511\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.06970129162073135, Val Loss: 2.2084035873413086, Val Acc: 88.85765910459747\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.07504837214946747, Val Loss: 2.2087619304656982, Val Acc: 88.75727765508934\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.07801032811403275, Val Loss: 2.2075703144073486, Val Acc: 88.6568962055812\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.08122087270021439, Val Loss: 2.2085883617401123, Val Acc: 88.45613330656495\n",
      "Epoch: [3/10], Step: [601/623], Train Loss: 0.08349931240081787, Val Loss: 2.2144079208374023, Val Acc: 87.69323429030315\n",
      "Epoch: [3/10], Train Acc: 99.24710134015962\n",
      "Epoch: [4/10], Step: [101/623], Train Loss: 0.03140495717525482, Val Loss: 2.208730459213257, Val Acc: 87.91407347922105\n",
      "Training stopped at epoch 4, iteration 101\n",
      "Max sentence length: 300 - Best Val Acc: 89.45994780164625\n"
     ]
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = 300\n",
    "\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "emb_dim = 50\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_sl300 = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_sl300.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Max sentence length: 300 - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_acc_sl100)\n",
    "plt.plot(val_acc_sl200)\n",
    "plt.plot(val_acc_sl300)\n",
    "plt.title('Max sentence length tuning - Validation Acc')\n",
    "plt.legend(['100', '200', '300'], loc='lower right')\n",
    "# plt.show()\n",
    "plt.savefig('Assignment_1/training_curve_maxlen.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max sentence length of 300 slightly outperforms 200 and 100. Using max sentence length of 300 from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/623], Train Loss: 0.9400739669799805, Val Loss: 2.4341659545898438, Val Acc: 77.4543264404738\n",
      "Epoch: [1/10], Step: [201/623], Train Loss: 0.6731591820716858, Val Loss: 2.301665782928467, Val Acc: 85.18369805259988\n",
      "Epoch: [1/10], Step: [301/623], Train Loss: 0.559447705745697, Val Loss: 2.2673938274383545, Val Acc: 86.60911463561534\n",
      "Epoch: [1/10], Step: [401/623], Train Loss: 0.4938570261001587, Val Loss: 2.248746395111084, Val Acc: 87.99437863882754\n",
      "Epoch: [1/10], Step: [501/623], Train Loss: 0.45787709951400757, Val Loss: 2.252855062484741, Val Acc: 87.83376831961453\n",
      "Epoch: [1/10], Step: [601/623], Train Loss: 0.4292944371700287, Val Loss: 2.2383973598480225, Val Acc: 89.17887974302349\n",
      "Epoch: [1/10], Train Acc: 95.13125533303217\n",
      "Epoch: [2/10], Step: [101/623], Train Loss: 0.15937727689743042, Val Loss: 2.225712776184082, Val Acc: 89.21903232282673\n",
      "Epoch: [2/10], Step: [201/623], Train Loss: 0.15636664628982544, Val Loss: 2.2194085121154785, Val Acc: 89.01826942381048\n",
      "Epoch: [2/10], Step: [301/623], Train Loss: 0.16083623468875885, Val Loss: 2.217911958694458, Val Acc: 89.29933748243324\n",
      "Epoch: [2/10], Step: [401/623], Train Loss: 0.15808440744876862, Val Loss: 2.215484142303467, Val Acc: 89.11865087331861\n",
      "Epoch: [2/10], Step: [501/623], Train Loss: 0.16261568665504456, Val Loss: 2.217764139175415, Val Acc: 88.6568962055812\n",
      "Epoch: [2/10], Step: [601/623], Train Loss: 0.16458627581596375, Val Loss: 2.217503786087036, Val Acc: 88.6568962055812\n",
      "Epoch: [2/10], Train Acc: 98.03744416001607\n",
      "Epoch: [3/10], Step: [101/623], Train Loss: 0.07807596772909164, Val Loss: 2.212494373321533, Val Acc: 88.71712507528609\n",
      "Epoch: [3/10], Step: [201/623], Train Loss: 0.07457978278398514, Val Loss: 2.2090651988983154, Val Acc: 88.6568962055812\n",
      "Epoch: [3/10], Step: [301/623], Train Loss: 0.07366307079792023, Val Loss: 2.210266590118408, Val Acc: 88.57659104597471\n",
      "Epoch: [3/10], Step: [401/623], Train Loss: 0.07752923667430878, Val Loss: 2.2143640518188477, Val Acc: 87.69323429030315\n",
      "Epoch: [3/10], Step: [501/623], Train Loss: 0.08182128518819809, Val Loss: 2.209643602371216, Val Acc: 88.51636217626982\n",
      "Epoch: [3/10], Step: [601/623], Train Loss: 0.08448801189661026, Val Loss: 2.2107105255126953, Val Acc: 88.31559927725357\n",
      "Epoch: [3/10], Train Acc: 99.44787431611705\n",
      "Epoch: [4/10], Step: [101/623], Train Loss: 0.032876431941986084, Val Loss: 2.207293748855591, Val Acc: 88.25537040754868\n",
      "Epoch: [4/10], Step: [201/623], Train Loss: 0.032496362924575806, Val Loss: 2.207362413406372, Val Acc: 88.19514153784381\n",
      "Training stopped at epoch 4, iteration 201\n",
      "Optimizer: Adam - Best Val Acc: 89.29933748243324\n"
     ]
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = 300\n",
    "\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "emb_dim = 50\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_adam = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_adam.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Optimizer: Adam - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/20], Step: [101/623], Train Loss: 2.6699931621551514, Val Loss: 2.9559168815612793, Val Acc: 50.6926320016061\n",
      "Epoch: [1/20], Step: [201/623], Train Loss: 2.4452199935913086, Val Loss: 2.9107539653778076, Val Acc: 52.61995583216222\n",
      "Epoch: [1/20], Step: [301/623], Train Loss: 2.2490530014038086, Val Loss: 2.861114263534546, Val Acc: 56.25376430435656\n",
      "Epoch: [1/20], Step: [401/623], Train Loss: 2.083353281021118, Val Loss: 2.813612699508667, Val Acc: 55.711704477012645\n",
      "Epoch: [1/20], Step: [501/623], Train Loss: 1.9440057277679443, Val Loss: 2.772538185119629, Val Acc: 56.71551897209396\n",
      "Epoch: [1/20], Step: [601/623], Train Loss: 1.827386498451233, Val Loss: 2.738725423812866, Val Acc: 56.81590042160209\n",
      "Epoch: [1/20], Train Acc: 57.345781257842695\n",
      "Epoch: [2/20], Step: [101/623], Train Loss: 1.1207168102264404, Val Loss: 2.706164598464966, Val Acc: 58.783376831961455\n",
      "Epoch: [2/20], Step: [201/623], Train Loss: 1.0851044654846191, Val Loss: 2.6854865550994873, Val Acc: 59.26520778960048\n",
      "Epoch: [2/20], Step: [301/623], Train Loss: 1.054688572883606, Val Loss: 2.6689069271087646, Val Acc: 59.52619955832162\n",
      "Epoch: [2/20], Step: [401/623], Train Loss: 1.0293768644332886, Val Loss: 2.6554386615753174, Val Acc: 59.80726761694439\n",
      "Epoch: [2/20], Step: [501/623], Train Loss: 1.007417917251587, Val Loss: 2.644219160079956, Val Acc: 59.8875727765509\n",
      "Epoch: [2/20], Step: [601/623], Train Loss: 0.9880145788192749, Val Loss: 2.634915828704834, Val Acc: 60.36940373418992\n",
      "Epoch: [2/20], Train Acc: 60.02108116247553\n",
      "Epoch: [3/20], Step: [101/623], Train Loss: 0.8663556575775146, Val Loss: 2.625438928604126, Val Acc: 60.449708893796426\n",
      "Epoch: [3/20], Step: [201/623], Train Loss: 0.8574426770210266, Val Loss: 2.618983030319214, Val Acc: 60.55009034330456\n",
      "Epoch: [3/20], Step: [301/623], Train Loss: 0.8489826917648315, Val Loss: 2.6133198738098145, Val Acc: 60.570166633206185\n",
      "Epoch: [3/20], Step: [401/623], Train Loss: 0.8417842388153076, Val Loss: 2.6084046363830566, Val Acc: 60.46978518369805\n",
      "Epoch: [3/20], Step: [501/623], Train Loss: 0.8349936008453369, Val Loss: 2.6040427684783936, Val Acc: 60.67054808271431\n",
      "Epoch: [3/20], Step: [601/623], Train Loss: 0.8285145163536072, Val Loss: 2.600162982940674, Val Acc: 61.05199759084521\n",
      "Epoch: [3/20], Train Acc: 61.01490739346484\n",
      "Epoch: [4/20], Step: [101/623], Train Loss: 0.7870042324066162, Val Loss: 2.595970630645752, Val Acc: 60.951616141337084\n",
      "Epoch: [4/20], Step: [201/623], Train Loss: 0.7838531136512756, Val Loss: 2.5928597450256348, Val Acc: 60.79100582212407\n",
      "Epoch: [4/20], Step: [301/623], Train Loss: 0.7802544236183167, Val Loss: 2.5901472568511963, Val Acc: 61.37321822927123\n",
      "Epoch: [4/20], Step: [401/623], Train Loss: 0.7767103910446167, Val Loss: 2.587627410888672, Val Acc: 61.453523388877734\n",
      "Epoch: [4/20], Step: [501/623], Train Loss: 0.7735568881034851, Val Loss: 2.585322141647339, Val Acc: 61.63420999799237\n",
      "Epoch: [4/20], Step: [601/623], Train Loss: 0.7708732485771179, Val Loss: 2.583247423171997, Val Acc: 61.81489660710701\n",
      "Epoch: [4/20], Train Acc: 61.662400240927575\n",
      "Epoch: [5/20], Step: [101/623], Train Loss: 0.7533461451530457, Val Loss: 2.5809009075164795, Val Acc: 61.7345914475005\n",
      "Epoch: [5/20], Step: [201/623], Train Loss: 0.7498021721839905, Val Loss: 2.5790364742279053, Val Acc: 61.774744027303754\n",
      "Epoch: [5/20], Step: [301/623], Train Loss: 0.7467395663261414, Val Loss: 2.577354669570923, Val Acc: 61.714515157598875\n",
      "Epoch: [5/20], Step: [401/623], Train Loss: 0.7443890571594238, Val Loss: 2.5758285522460938, Val Acc: 62.01565950612327\n",
      "Epoch: [5/20], Step: [501/623], Train Loss: 0.7426206469535828, Val Loss: 2.5743489265441895, Val Acc: 61.915278056615136\n",
      "Epoch: [5/20], Step: [601/623], Train Loss: 0.7412852644920349, Val Loss: 2.5730180740356445, Val Acc: 61.79482031720538\n",
      "Epoch: [5/20], Train Acc: 61.96857902926266\n",
      "Epoch: [6/20], Step: [101/623], Train Loss: 0.7294874787330627, Val Loss: 2.571410655975342, Val Acc: 61.85504918691026\n",
      "Epoch: [6/20], Step: [201/623], Train Loss: 0.7278200387954712, Val Loss: 2.570173501968384, Val Acc: 61.915278056615136\n",
      "Epoch: [6/20], Step: [301/623], Train Loss: 0.7266876101493835, Val Loss: 2.5690770149230957, Val Acc: 62.1963461152379\n",
      "Epoch: [6/20], Step: [401/623], Train Loss: 0.725789487361908, Val Loss: 2.5679872035980225, Val Acc: 62.095964665729774\n",
      "Epoch: [6/20], Step: [501/623], Train Loss: 0.7238403558731079, Val Loss: 2.566879987716675, Val Acc: 62.095964665729774\n",
      "Epoch: [6/20], Step: [601/623], Train Loss: 0.7229883670806885, Val Loss: 2.5659446716308594, Val Acc: 62.05581208592652\n",
      "Epoch: [6/20], Train Acc: 62.17939065401797\n",
      "Epoch: [7/20], Step: [101/623], Train Loss: 0.7141488790512085, Val Loss: 2.5647099018096924, Val Acc: 62.31680385464766\n",
      "Epoch: [7/20], Step: [201/623], Train Loss: 0.7118985652923584, Val Loss: 2.563828229904175, Val Acc: 62.25657498494278\n",
      "Epoch: [7/20], Step: [301/623], Train Loss: 0.7119331359863281, Val Loss: 2.56296443939209, Val Acc: 62.1160409556314\n",
      "Epoch: [7/20], Step: [401/623], Train Loss: 0.712346613407135, Val Loss: 2.5621814727783203, Val Acc: 62.17626982533628\n",
      "Epoch: [7/20], Step: [501/623], Train Loss: 0.7104917168617249, Val Loss: 2.5613324642181396, Val Acc: 62.1963461152379\n",
      "Epoch: [7/20], Step: [601/623], Train Loss: 0.7096273899078369, Val Loss: 2.560558319091797, Val Acc: 62.1963461152379\n",
      "Epoch: [7/20], Train Acc: 62.58093660593284\n",
      "Epoch: [8/20], Step: [101/623], Train Loss: 0.7024555802345276, Val Loss: 2.5596423149108887, Val Acc: 62.1963461152379\n",
      "Epoch: [8/20], Step: [201/623], Train Loss: 0.7039518356323242, Val Loss: 2.558927059173584, Val Acc: 62.27665127484441\n",
      "Epoch: [8/20], Step: [301/623], Train Loss: 0.7035311460494995, Val Loss: 2.5582492351531982, Val Acc: 62.31680385464766\n",
      "Epoch: [8/20], Step: [401/623], Train Loss: 0.7021803855895996, Val Loss: 2.55753493309021, Val Acc: 62.236498695041156\n",
      "Epoch: [8/20], Step: [501/623], Train Loss: 0.7010737061500549, Val Loss: 2.556874990463257, Val Acc: 62.27665127484441\n",
      "Epoch: [8/20], Step: [601/623], Train Loss: 0.7002541422843933, Val Loss: 2.556241512298584, Val Acc: 62.43726159405742\n",
      "Epoch: [8/20], Train Acc: 62.781709581890276\n",
      "Epoch: [9/20], Step: [101/623], Train Loss: 0.693706750869751, Val Loss: 2.555485248565674, Val Acc: 62.095964665729774\n",
      "Epoch: [9/20], Step: [201/623], Train Loss: 0.6937020421028137, Val Loss: 2.554987668991089, Val Acc: 62.236498695041156\n",
      "Epoch: [9/20], Step: [301/623], Train Loss: 0.6935860514640808, Val Loss: 2.5543665885925293, Val Acc: 62.25657498494278\n",
      "Epoch: [9/20], Step: [401/623], Train Loss: 0.6932599544525146, Val Loss: 2.553800106048584, Val Acc: 62.33688014454929\n",
      "Epoch: [9/20], Step: [501/623], Train Loss: 0.6929823160171509, Val Loss: 2.5532264709472656, Val Acc: 62.296727564746035\n",
      "Epoch: [9/20], Step: [601/623], Train Loss: 0.692905604839325, Val Loss: 2.552717924118042, Val Acc: 62.296727564746035\n",
      "Epoch: [9/20], Train Acc: 62.86703809667219\n",
      "Epoch: [10/20], Step: [101/623], Train Loss: 0.6870318651199341, Val Loss: 2.5520927906036377, Val Acc: 62.356956434450915\n",
      "Epoch: [10/20], Step: [201/623], Train Loss: 0.6872209310531616, Val Loss: 2.551563024520874, Val Acc: 62.4974904637623\n",
      "Epoch: [10/20], Step: [301/623], Train Loss: 0.6871144771575928, Val Loss: 2.551079273223877, Val Acc: 62.4974904637623\n",
      "Epoch: [10/20], Step: [401/623], Train Loss: 0.6863475441932678, Val Loss: 2.5505006313323975, Val Acc: 62.53764304356555\n",
      "Epoch: [10/20], Step: [501/623], Train Loss: 0.6864503622055054, Val Loss: 2.550083637237549, Val Acc: 62.6581007829753\n",
      "Epoch: [10/20], Step: [601/623], Train Loss: 0.6865164041519165, Val Loss: 2.5497162342071533, Val Acc: 62.63802449307368\n",
      "Epoch: [10/20], Train Acc: 63.007579179842395\n",
      "Epoch: [11/20], Step: [101/623], Train Loss: 0.6799762845039368, Val Loss: 2.5491039752960205, Val Acc: 62.63802449307368\n",
      "Epoch: [11/20], Step: [201/623], Train Loss: 0.6810641288757324, Val Loss: 2.548642158508301, Val Acc: 62.63802449307368\n",
      "Epoch: [11/20], Step: [301/623], Train Loss: 0.6806876063346863, Val Loss: 2.5482139587402344, Val Acc: 62.617948203172055\n",
      "Epoch: [11/20], Step: [401/623], Train Loss: 0.681649923324585, Val Loss: 2.547839879989624, Val Acc: 62.75848223248344\n",
      "Epoch: [11/20], Step: [501/623], Train Loss: 0.6806610822677612, Val Loss: 2.5473010540008545, Val Acc: 62.6581007829753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11/20], Step: [601/623], Train Loss: 0.6808881163597107, Val Loss: 2.5469400882720947, Val Acc: 62.4974904637623\n",
      "Epoch: [11/20], Train Acc: 63.10796566782111\n",
      "Epoch: [12/20], Step: [101/623], Train Loss: 0.6770086288452148, Val Loss: 2.546562671661377, Val Acc: 62.4974904637623\n",
      "Epoch: [12/20], Step: [201/623], Train Loss: 0.6780239343643188, Val Loss: 2.5460925102233887, Val Acc: 62.27665127484441\n",
      "Epoch: [12/20], Step: [301/623], Train Loss: 0.6774070858955383, Val Loss: 2.545693874359131, Val Acc: 62.356956434450915\n",
      "Epoch: [12/20], Step: [401/623], Train Loss: 0.678369402885437, Val Loss: 2.545280933380127, Val Acc: 62.5777956233688\n",
      "Epoch: [12/20], Step: [501/623], Train Loss: 0.6774892807006836, Val Loss: 2.544933557510376, Val Acc: 62.71832965268018\n",
      "Epoch: [12/20], Step: [601/623], Train Loss: 0.6769669055938721, Val Loss: 2.544534206390381, Val Acc: 62.71832965268018\n",
      "Epoch: [12/20], Train Acc: 63.25854539978919\n",
      "Epoch: [13/20], Step: [101/623], Train Loss: 0.6765989661216736, Val Loss: 2.5441317558288574, Val Acc: 62.858863681991565\n",
      "Epoch: [13/20], Step: [201/623], Train Loss: 0.6760281324386597, Val Loss: 2.543689250946045, Val Acc: 62.617948203172055\n",
      "Epoch: [13/20], Step: [301/623], Train Loss: 0.674694836139679, Val Loss: 2.543464422225952, Val Acc: 62.79863481228669\n",
      "Epoch: [13/20], Step: [401/623], Train Loss: 0.6742785573005676, Val Loss: 2.543059825897217, Val Acc: 62.919092551696444\n",
      "Epoch: [13/20], Step: [501/623], Train Loss: 0.6737291812896729, Val Loss: 2.542734384536743, Val Acc: 62.81871110218832\n",
      "Epoch: [13/20], Step: [601/623], Train Loss: 0.6731736063957214, Val Loss: 2.542372941970825, Val Acc: 62.678177072876935\n",
      "Epoch: [13/20], Train Acc: 63.198313507001956\n",
      "Epoch: [14/20], Step: [101/623], Train Loss: 0.6784756183624268, Val Loss: 2.5420048236846924, Val Acc: 62.858863681991565\n",
      "Epoch: [14/20], Step: [201/623], Train Loss: 0.673639714717865, Val Loss: 2.5417325496673584, Val Acc: 62.89901626179482\n",
      "Epoch: [14/20], Step: [301/623], Train Loss: 0.6722583770751953, Val Loss: 2.541417121887207, Val Acc: 62.557719333467176\n",
      "Epoch: [14/20], Step: [401/623], Train Loss: 0.6722725629806519, Val Loss: 2.541051149368286, Val Acc: 62.999397711302954\n",
      "Epoch: [14/20], Step: [501/623], Train Loss: 0.6700447201728821, Val Loss: 2.540663957595825, Val Acc: 62.97932142140132\n",
      "Epoch: [14/20], Step: [601/623], Train Loss: 0.6694929003715515, Val Loss: 2.5404186248779297, Val Acc: 62.69825336277856\n",
      "Epoch: [14/20], Train Acc: 63.52958891733173\n",
      "Epoch: [15/20], Step: [101/623], Train Loss: 0.6720094084739685, Val Loss: 2.540060043334961, Val Acc: 62.39710901425416\n",
      "Epoch: [15/20], Step: [201/623], Train Loss: 0.6684373021125793, Val Loss: 2.539745569229126, Val Acc: 62.89901626179482\n",
      "Epoch: [15/20], Step: [301/623], Train Loss: 0.6680880188941956, Val Loss: 2.5393574237823486, Val Acc: 62.97932142140132\n",
      "Epoch: [15/20], Step: [401/623], Train Loss: 0.6673902273178101, Val Loss: 2.539022922515869, Val Acc: 62.999397711302954\n",
      "Epoch: [15/20], Step: [501/623], Train Loss: 0.6663782000541687, Val Loss: 2.5387938022613525, Val Acc: 62.81871110218832\n",
      "Epoch: [15/20], Step: [601/623], Train Loss: 0.6663796305656433, Val Loss: 2.5385096073150635, Val Acc: 62.81871110218832\n",
      "Epoch: [15/20], Train Acc: 63.655072027305124\n",
      "Epoch: [16/20], Step: [101/623], Train Loss: 0.669153094291687, Val Loss: 2.5381927490234375, Val Acc: 63.01947400120458\n",
      "Epoch: [16/20], Step: [201/623], Train Loss: 0.6681751012802124, Val Loss: 2.537891149520874, Val Acc: 62.89901626179482\n",
      "Epoch: [16/20], Step: [301/623], Train Loss: 0.6648445129394531, Val Loss: 2.5375640392303467, Val Acc: 63.07970287090946\n",
      "Epoch: [16/20], Step: [401/623], Train Loss: 0.6639211773872375, Val Loss: 2.537242889404297, Val Acc: 63.09977916081108\n",
      "Epoch: [16/20], Step: [501/623], Train Loss: 0.6635400056838989, Val Loss: 2.5370006561279297, Val Acc: 62.9592451314997\n",
      "Epoch: [16/20], Step: [601/623], Train Loss: 0.6637359857559204, Val Loss: 2.5366785526275635, Val Acc: 63.119855450712706\n",
      "Epoch: [16/20], Train Acc: 63.81569040807107\n",
      "Epoch: [17/20], Step: [101/623], Train Loss: 0.6624537706375122, Val Loss: 2.536432981491089, Val Acc: 63.28046576992572\n",
      "Epoch: [17/20], Step: [201/623], Train Loss: 0.6615286469459534, Val Loss: 2.53611159324646, Val Acc: 63.01947400120458\n",
      "Epoch: [17/20], Step: [301/623], Train Loss: 0.6626085638999939, Val Loss: 2.535853147506714, Val Acc: 63.22023690022084\n",
      "Epoch: [17/20], Step: [401/623], Train Loss: 0.6611781716346741, Val Loss: 2.535592794418335, Val Acc: 63.380847219433846\n",
      "Epoch: [17/20], Step: [501/623], Train Loss: 0.660474956035614, Val Loss: 2.5354206562042236, Val Acc: 63.36077092953222\n",
      "Epoch: [17/20], Step: [601/623], Train Loss: 0.6607176065444946, Val Loss: 2.5350182056427, Val Acc: 63.36077092953222\n",
      "Epoch: [17/20], Train Acc: 63.921096220448725\n",
      "Epoch: [18/20], Step: [101/623], Train Loss: 0.6582610607147217, Val Loss: 2.53481388092041, Val Acc: 63.30054205982734\n",
      "Epoch: [18/20], Step: [201/623], Train Loss: 0.6600505709648132, Val Loss: 2.534569025039673, Val Acc: 63.380847219433846\n",
      "Epoch: [18/20], Step: [301/623], Train Loss: 0.6591625213623047, Val Loss: 2.534252166748047, Val Acc: 63.119855450712706\n",
      "Epoch: [18/20], Step: [401/623], Train Loss: 0.6603882312774658, Val Loss: 2.53406023979187, Val Acc: 63.380847219433846\n",
      "Epoch: [18/20], Step: [501/623], Train Loss: 0.65920490026474, Val Loss: 2.533803701400757, Val Acc: 63.180084320417585\n",
      "Epoch: [18/20], Step: [601/623], Train Loss: 0.6590560674667358, Val Loss: 2.533582925796509, Val Acc: 63.09977916081108\n",
      "Epoch: [18/20], Train Acc: 63.90603824725192\n",
      "Epoch: [19/20], Step: [101/623], Train Loss: 0.6547926068305969, Val Loss: 2.533193349838257, Val Acc: 63.40092350933548\n",
      "Epoch: [19/20], Step: [201/623], Train Loss: 0.6585654020309448, Val Loss: 2.532961130142212, Val Acc: 63.40092350933548\n",
      "Epoch: [19/20], Step: [301/623], Train Loss: 0.6581783890724182, Val Loss: 2.5326907634735107, Val Acc: 63.240313190122464\n",
      "Epoch: [19/20], Step: [401/623], Train Loss: 0.6571361422538757, Val Loss: 2.532489776611328, Val Acc: 63.16000803051596\n",
      "Epoch: [19/20], Step: [501/623], Train Loss: 0.65708327293396, Val Loss: 2.5323164463043213, Val Acc: 63.441076089138726\n",
      "Epoch: [19/20], Step: [601/623], Train Loss: 0.6562555432319641, Val Loss: 2.5320277214050293, Val Acc: 63.3406946396306\n",
      "Epoch: [19/20], Train Acc: 64.14194649400191\n",
      "Epoch: [20/20], Step: [101/623], Train Loss: 0.6581432819366455, Val Loss: 2.5317931175231934, Val Acc: 63.380847219433846\n",
      "Epoch: [20/20], Step: [201/623], Train Loss: 0.6554219722747803, Val Loss: 2.531566858291626, Val Acc: 63.28046576992572\n",
      "Epoch: [20/20], Step: [301/623], Train Loss: 0.6558414697647095, Val Loss: 2.5312764644622803, Val Acc: 63.240313190122464\n",
      "Epoch: [20/20], Step: [401/623], Train Loss: 0.6549292802810669, Val Loss: 2.53108811378479, Val Acc: 63.32061834972897\n",
      "Epoch: [20/20], Step: [501/623], Train Loss: 0.65419602394104, Val Loss: 2.530954122543335, Val Acc: 63.48122866894198\n",
      "Epoch: [20/20], Step: [601/623], Train Loss: 0.6542139053344727, Val Loss: 2.5306146144866943, Val Acc: 63.3406946396306\n",
      "Epoch: [20/20], Train Acc: 64.15198514279977\n",
      "Optimizer: SGD - Best Val Acc: 63.48122866894198\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 50\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_sgd = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_sgd.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Optimizer: SGD - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_acc_adam)\n",
    "plt.plot(val_acc_sgd)\n",
    "plt.title('Optimizer tuning - Validation Acc')\n",
    "plt.legend(['Adam', 'SGD'], loc='lower right')\n",
    "# plt.show()\n",
    "plt.savefig('Assignment_1/training_curve_opt.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Adam outperforms SGD in the first 20 epoches. The learning curve of SGD flattens after epoch 3. Using Adam from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. constant 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/20], Step: [101/623], Train Loss: 0.955021858215332, Val Loss: 2.4586009979248047, Val Acc: 77.59486046978519\n",
      "Epoch: [1/20], Step: [201/623], Train Loss: 0.6930336356163025, Val Loss: 2.3105239868164062, Val Acc: 85.74583416984541\n",
      "Epoch: [1/20], Step: [301/623], Train Loss: 0.5709260702133179, Val Loss: 2.2677316665649414, Val Acc: 87.61292913069664\n",
      "Epoch: [1/20], Step: [401/623], Train Loss: 0.5068856477737427, Val Loss: 2.2488317489624023, Val Acc: 88.67697249548283\n",
      "Epoch: [1/20], Step: [501/623], Train Loss: 0.4617069363594055, Val Loss: 2.237438917160034, Val Acc: 88.97811684400723\n",
      "Epoch: [1/20], Step: [601/623], Train Loss: 0.4335377812385559, Val Loss: 2.234494686126709, Val Acc: 89.60048183095763\n",
      "Epoch: [1/20], Train Acc: 95.02083019625559\n",
      "Epoch: [2/20], Step: [101/623], Train Loss: 0.15438780188560486, Val Loss: 2.2238731384277344, Val Acc: 88.95804055410561\n",
      "Epoch: [2/20], Step: [201/623], Train Loss: 0.15804074704647064, Val Loss: 2.2169713973999023, Val Acc: 89.05842200361373\n",
      "Epoch: [2/20], Step: [301/623], Train Loss: 0.16061653196811676, Val Loss: 2.2155349254608154, Val Acc: 89.56032925115439\n",
      "Epoch: [2/20], Step: [401/623], Train Loss: 0.16294054687023163, Val Loss: 2.2137701511383057, Val Acc: 89.54025296125276\n",
      "Epoch: [2/20], Step: [501/623], Train Loss: 0.16233576834201813, Val Loss: 2.212893009185791, Val Acc: 89.29933748243324\n",
      "Epoch: [2/20], Step: [601/623], Train Loss: 0.16569074988365173, Val Loss: 2.213326930999756, Val Acc: 88.99819313390886\n",
      "Epoch: [2/20], Train Acc: 97.97721226722884\n",
      "Epoch: [3/20], Step: [101/623], Train Loss: 0.07752712815999985, Val Loss: 2.211200475692749, Val Acc: 88.8777353944991\n",
      "Epoch: [3/20], Step: [201/623], Train Loss: 0.07320744544267654, Val Loss: 2.2075998783111572, Val Acc: 89.17887974302349\n",
      "Epoch: [3/20], Step: [301/623], Train Loss: 0.0755983293056488, Val Loss: 2.2088513374328613, Val Acc: 88.85765910459747\n",
      "Epoch: [3/20], Step: [401/623], Train Loss: 0.08045540750026703, Val Loss: 2.2101151943206787, Val Acc: 88.33567556715519\n",
      "Epoch: [3/20], Step: [501/623], Train Loss: 0.08441220223903656, Val Loss: 2.2094595432281494, Val Acc: 88.47620959646657\n",
      "Training stopped at epoch 3, iteration 501\n",
      "Learning rate: 0.01 - Best Val Acc: 89.60048183095763\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 50\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_lr01 = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_lr01.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Learning rate: 0.01 - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. constant 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/20], Step: [101/623], Train Loss: 2.6216535568237305, Val Loss: 2.910858631134033, Val Acc: 49.98996185504919\n",
      "Epoch: [1/20], Step: [201/623], Train Loss: 2.0189640522003174, Val Loss: 2.6594908237457275, Val Acc: 50.31118249347521\n",
      "Epoch: [1/20], Step: [301/623], Train Loss: 1.6238374710083008, Val Loss: 2.5841898918151855, Val Acc: 60.16864083517366\n",
      "Epoch: [1/20], Step: [401/623], Train Loss: 1.4002329111099243, Val Loss: 2.5607619285583496, Val Acc: 63.561533828548484\n",
      "Epoch: [1/20], Step: [501/623], Train Loss: 1.258457064628601, Val Loss: 2.545056104660034, Val Acc: 69.80525998795423\n",
      "Epoch: [1/20], Step: [601/623], Train Loss: 1.1584316492080688, Val Loss: 2.528850793838501, Val Acc: 73.6197550692632\n",
      "Epoch: [1/20], Train Acc: 75.38021382321939\n",
      "Epoch: [2/20], Step: [101/623], Train Loss: 0.6157978773117065, Val Loss: 2.5049095153808594, Val Acc: 76.28990162617949\n",
      "Epoch: [2/20], Step: [201/623], Train Loss: 0.5952032804489136, Val Loss: 2.482261896133423, Val Acc: 78.31760690624373\n",
      "Epoch: [2/20], Step: [301/623], Train Loss: 0.5775406956672668, Val Loss: 2.4584107398986816, Val Acc: 79.04035334270228\n",
      "Epoch: [2/20], Step: [401/623], Train Loss: 0.5597683191299438, Val Loss: 2.4348247051239014, Val Acc: 81.00782975306163\n",
      "Epoch: [2/20], Step: [501/623], Train Loss: 0.5431114435195923, Val Loss: 2.413419723510742, Val Acc: 81.89118650873318\n",
      "Epoch: [2/20], Step: [601/623], Train Loss: 0.5269643664360046, Val Loss: 2.3934764862060547, Val Acc: 82.57378036538847\n",
      "Epoch: [2/20], Train Acc: 85.87060181699543\n",
      "Epoch: [3/20], Step: [101/623], Train Loss: 0.4064246416091919, Val Loss: 2.3718669414520264, Val Acc: 83.85866291909255\n",
      "Epoch: [3/20], Step: [201/623], Train Loss: 0.3875986635684967, Val Loss: 2.3558552265167236, Val Acc: 84.38064645653483\n",
      "Epoch: [3/20], Step: [301/623], Train Loss: 0.3797400891780853, Val Loss: 2.3423264026641846, Val Acc: 84.84240112427223\n",
      "Epoch: [3/20], Step: [401/623], Train Loss: 0.372253954410553, Val Loss: 2.3308122158050537, Val Acc: 85.46476611122264\n",
      "Epoch: [3/20], Step: [501/623], Train Loss: 0.36336374282836914, Val Loss: 2.3196964263916016, Val Acc: 85.9666733587633\n",
      "Epoch: [3/20], Step: [601/623], Train Loss: 0.356920063495636, Val Loss: 2.3112905025482178, Val Acc: 86.40835173659907\n",
      "Epoch: [3/20], Train Acc: 89.65015308939417\n",
      "Epoch: [4/20], Step: [101/623], Train Loss: 0.297541081905365, Val Loss: 2.3019726276397705, Val Acc: 86.58903834571372\n",
      "Epoch: [4/20], Step: [201/623], Train Loss: 0.2989520728588104, Val Loss: 2.2951982021331787, Val Acc: 86.99056414374624\n",
      "Epoch: [4/20], Step: [301/623], Train Loss: 0.2935912311077118, Val Loss: 2.2894415855407715, Val Acc: 87.13109817305762\n",
      "Epoch: [4/20], Step: [401/623], Train Loss: 0.2848680913448334, Val Loss: 2.2834837436676025, Val Acc: 87.25155591246738\n",
      "Epoch: [4/20], Step: [501/623], Train Loss: 0.2791743278503418, Val Loss: 2.2777810096740723, Val Acc: 87.5727765508934\n",
      "Epoch: [4/20], Step: [601/623], Train Loss: 0.2751762866973877, Val Loss: 2.27341628074646, Val Acc: 87.67315800040153\n",
      "Epoch: [4/20], Train Acc: 91.8084625809366\n",
      "Epoch: [5/20], Step: [101/623], Train Loss: 0.23394151031970978, Val Loss: 2.2686595916748047, Val Acc: 87.93414976912267\n",
      "Epoch: [5/20], Step: [201/623], Train Loss: 0.23223549127578735, Val Loss: 2.264286994934082, Val Acc: 88.07468379843405\n",
      "Epoch: [5/20], Step: [301/623], Train Loss: 0.22990727424621582, Val Loss: 2.260704278945923, Val Acc: 88.17506524794219\n",
      "Epoch: [5/20], Step: [401/623], Train Loss: 0.22714227437973022, Val Loss: 2.257657766342163, Val Acc: 88.43605701666333\n",
      "Epoch: [5/20], Step: [501/623], Train Loss: 0.22758115828037262, Val Loss: 2.254995822906494, Val Acc: 88.43605701666333\n",
      "Epoch: [5/20], Step: [601/623], Train Loss: 0.22701925039291382, Val Loss: 2.2524406909942627, Val Acc: 88.57659104597471\n",
      "Epoch: [5/20], Train Acc: 93.35441449580887\n",
      "Epoch: [6/20], Step: [101/623], Train Loss: 0.1923552006483078, Val Loss: 2.2499642372131348, Val Acc: 88.63681991567958\n",
      "Epoch: [6/20], Step: [201/623], Train Loss: 0.18644706904888153, Val Loss: 2.2464439868927, Val Acc: 88.77735394499096\n",
      "Epoch: [6/20], Step: [301/623], Train Loss: 0.18763375282287598, Val Loss: 2.2447004318237305, Val Acc: 88.75727765508934\n",
      "Epoch: [6/20], Step: [401/623], Train Loss: 0.19134190678596497, Val Loss: 2.2421305179595947, Val Acc: 88.95804055410561\n",
      "Epoch: [6/20], Step: [501/623], Train Loss: 0.19217301905155182, Val Loss: 2.240327835083008, Val Acc: 88.97811684400723\n",
      "Epoch: [6/20], Step: [601/623], Train Loss: 0.1939430832862854, Val Loss: 2.239534854888916, Val Acc: 88.99819313390886\n",
      "Epoch: [6/20], Train Acc: 94.45866586357477\n",
      "Epoch: [7/20], Step: [101/623], Train Loss: 0.17316453158855438, Val Loss: 2.23726487159729, Val Acc: 88.89781168440072\n",
      "Epoch: [7/20], Step: [201/623], Train Loss: 0.16820983588695526, Val Loss: 2.2356245517730713, Val Acc: 88.91788797430235\n",
      "Epoch: [7/20], Step: [301/623], Train Loss: 0.16693328320980072, Val Loss: 2.2338128089904785, Val Acc: 88.99819313390886\n",
      "Epoch: [7/20], Step: [401/623], Train Loss: 0.16506992280483246, Val Loss: 2.23250675201416, Val Acc: 89.15880345312186\n",
      "Epoch: [7/20], Step: [501/623], Train Loss: 0.16654151678085327, Val Loss: 2.2315661907196045, Val Acc: 89.07849829351535\n",
      "Epoch: [7/20], Step: [601/623], Train Loss: 0.16631947457790375, Val Loss: 2.230107307434082, Val Acc: 89.21903232282673\n",
      "Epoch: [7/20], Train Acc: 95.51774331175024\n",
      "Epoch: [8/20], Step: [101/623], Train Loss: 0.14534448087215424, Val Loss: 2.229058265686035, Val Acc: 89.37964264203976\n",
      "Epoch: [8/20], Step: [201/623], Train Loss: 0.14387613534927368, Val Loss: 2.227060079574585, Val Acc: 89.45994780164625\n",
      "Epoch: [8/20], Step: [301/623], Train Loss: 0.14157405495643616, Val Loss: 2.2259328365325928, Val Acc: 89.419795221843\n",
      "Epoch: [8/20], Step: [401/623], Train Loss: 0.144272118806839, Val Loss: 2.225046396255493, Val Acc: 89.29933748243324\n",
      "Epoch: [8/20], Step: [501/623], Train Loss: 0.14440400898456573, Val Loss: 2.224669933319092, Val Acc: 89.419795221843\n",
      "Epoch: [8/20], Step: [601/623], Train Loss: 0.1440843939781189, Val Loss: 2.223393678665161, Val Acc: 89.48002409154788\n",
      "Epoch: [8/20], Train Acc: 96.4061637303619\n",
      "Epoch: [9/20], Step: [101/623], Train Loss: 0.12054675817489624, Val Loss: 2.2221195697784424, Val Acc: 89.45994780164625\n",
      "Epoch: [9/20], Step: [201/623], Train Loss: 0.1261151134967804, Val Loss: 2.2219135761260986, Val Acc: 89.58040554105601\n",
      "Epoch: [9/20], Step: [301/623], Train Loss: 0.1265047937631607, Val Loss: 2.2207255363464355, Val Acc: 89.52017667135114\n",
      "Epoch: [9/20], Step: [401/623], Train Loss: 0.1236407533288002, Val Loss: 2.2197577953338623, Val Acc: 89.62055812085927\n",
      "Epoch: [9/20], Step: [501/623], Train Loss: 0.12365387380123138, Val Loss: 2.2192234992980957, Val Acc: 89.56032925115439\n",
      "Epoch: [9/20], Step: [601/623], Train Loss: 0.12394092231988907, Val Loss: 2.21886944770813, Val Acc: 89.56032925115439\n",
      "Epoch: [9/20], Train Acc: 96.94323144104804\n",
      "Epoch: [10/20], Step: [101/623], Train Loss: 0.10725799202919006, Val Loss: 2.2180211544036865, Val Acc: 89.52017667135114\n",
      "Epoch: [10/20], Step: [201/623], Train Loss: 0.10890735685825348, Val Loss: 2.21728515625, Val Acc: 89.60048183095763\n",
      "Epoch: [10/20], Step: [301/623], Train Loss: 0.10928303003311157, Val Loss: 2.216926097869873, Val Acc: 89.54025296125276\n",
      "Epoch: [10/20], Step: [401/623], Train Loss: 0.10778121650218964, Val Loss: 2.2162578105926514, Val Acc: 89.58040554105601\n",
      "Epoch: [10/20], Step: [501/623], Train Loss: 0.10554207116365433, Val Loss: 2.215806722640991, Val Acc: 89.48002409154788\n",
      "Epoch: [10/20], Step: [601/623], Train Loss: 0.10636471956968307, Val Loss: 2.2149906158447266, Val Acc: 89.70086328046577\n",
      "Epoch: [10/20], Train Acc: 97.70114942528735\n",
      "Epoch: [11/20], Step: [101/623], Train Loss: 0.08688701689243317, Val Loss: 2.2146997451782227, Val Acc: 89.76109215017065\n",
      "Epoch: [11/20], Step: [201/623], Train Loss: 0.08820439875125885, Val Loss: 2.214372396469116, Val Acc: 89.60048183095763\n",
      "Epoch: [11/20], Step: [301/623], Train Loss: 0.08941881358623505, Val Loss: 2.2135136127471924, Val Acc: 89.52017667135114\n",
      "Epoch: [11/20], Step: [401/623], Train Loss: 0.09097904711961746, Val Loss: 2.2135064601898193, Val Acc: 89.58040554105601\n",
      "Epoch: [11/20], Step: [501/623], Train Loss: 0.09142974764108658, Val Loss: 2.212862730026245, Val Acc: 89.72093957036739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11/20], Step: [601/623], Train Loss: 0.0917932540178299, Val Loss: 2.2123425006866455, Val Acc: 89.419795221843\n",
      "Epoch: [11/20], Train Acc: 98.26331375796818\n",
      "Epoch: [12/20], Step: [101/623], Train Loss: 0.07840949296951294, Val Loss: 2.211923837661743, Val Acc: 89.72093957036739\n",
      "Epoch: [12/20], Step: [201/623], Train Loss: 0.07794946432113647, Val Loss: 2.211240768432617, Val Acc: 89.76109215017065\n",
      "Epoch: [12/20], Step: [301/623], Train Loss: 0.079647496342659, Val Loss: 2.2112905979156494, Val Acc: 89.78116844007228\n",
      "Epoch: [12/20], Step: [401/623], Train Loss: 0.07898058742284775, Val Loss: 2.2107186317443848, Val Acc: 89.66071070066252\n",
      "Epoch: [12/20], Step: [501/623], Train Loss: 0.07877819240093231, Val Loss: 2.210475444793701, Val Acc: 89.62055812085927\n",
      "Epoch: [12/20], Step: [601/623], Train Loss: 0.07883724570274353, Val Loss: 2.21016788482666, Val Acc: 89.58040554105601\n",
      "Epoch: [12/20], Train Acc: 98.6999949806756\n",
      "Epoch: [13/20], Step: [101/623], Train Loss: 0.06368532031774521, Val Loss: 2.2094805240631104, Val Acc: 89.50010038144951\n",
      "Epoch: [13/20], Step: [201/623], Train Loss: 0.06352663040161133, Val Loss: 2.209106683731079, Val Acc: 89.58040554105601\n",
      "Epoch: [13/20], Step: [301/623], Train Loss: 0.06619738042354584, Val Loss: 2.2093024253845215, Val Acc: 89.52017667135114\n",
      "Epoch: [13/20], Step: [401/623], Train Loss: 0.0653054416179657, Val Loss: 2.2085177898406982, Val Acc: 89.62055812085927\n",
      "Epoch: [13/20], Step: [501/623], Train Loss: 0.06566362082958221, Val Loss: 2.208420991897583, Val Acc: 89.35956635213813\n",
      "Epoch: [13/20], Step: [601/623], Train Loss: 0.06640078127384186, Val Loss: 2.208338975906372, Val Acc: 89.56032925115439\n",
      "Epoch: [13/20], Train Acc: 99.07142498619686\n",
      "Epoch: [14/20], Step: [101/623], Train Loss: 0.05645393207669258, Val Loss: 2.208022356033325, Val Acc: 89.48002409154788\n",
      "Epoch: [14/20], Step: [201/623], Train Loss: 0.05719603970646858, Val Loss: 2.2081775665283203, Val Acc: 89.48002409154788\n",
      "Training stopped at epoch 14, iteration 201\n",
      "Learning rate: 0.001 - Best Val Acc: 89.78116844007228\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 50\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_lr001 = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_lr001.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Learning rate: 0.001 - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. constant 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/50], Step: [101/623], Train Loss: 2.9809176921844482, Val Loss: 2.992307662963867, Val Acc: 20.21682393093756\n",
      "Epoch: [1/50], Step: [201/623], Train Loss: 2.9429097175598145, Val Loss: 2.9876184463500977, Val Acc: 33.26641236699458\n",
      "Epoch: [1/50], Step: [301/623], Train Loss: 2.9032437801361084, Val Loss: 2.981966018676758, Val Acc: 42.64203975105401\n",
      "Epoch: [1/50], Step: [401/623], Train Loss: 2.8609225749969482, Val Loss: 2.974994421005249, Val Acc: 47.52057819714917\n",
      "Epoch: [1/50], Step: [501/623], Train Loss: 2.8143887519836426, Val Loss: 2.966266393661499, Val Acc: 49.46797831760691\n",
      "Epoch: [1/50], Step: [601/623], Train Loss: 2.763838052749634, Val Loss: 2.9554481506347656, Val Acc: 50.31118249347521\n",
      "Epoch: [1/50], Train Acc: 50.248456557747325\n",
      "Epoch: [2/50], Step: [101/623], Train Loss: 2.357910633087158, Val Loss: 2.9387550354003906, Val Acc: 50.93354748042562\n",
      "Epoch: [2/50], Step: [201/623], Train Loss: 2.288923501968384, Val Loss: 2.9219934940338135, Val Acc: 51.03392892993375\n",
      "Epoch: [2/50], Step: [301/623], Train Loss: 2.2176096439361572, Val Loss: 2.9021553993225098, Val Acc: 51.23469182895001\n",
      "Epoch: [2/50], Step: [401/623], Train Loss: 2.1449224948883057, Val Loss: 2.8793954849243164, Val Acc: 51.23469182895001\n",
      "Epoch: [2/50], Step: [501/623], Train Loss: 2.0727810859680176, Val Loss: 2.8541619777679443, Val Acc: 51.23469182895001\n",
      "Epoch: [2/50], Step: [601/623], Train Loss: 2.000715970993042, Val Loss: 2.827225685119629, Val Acc: 51.33507327845814\n",
      "Epoch: [2/50], Train Acc: 51.44807508909301\n",
      "Epoch: [3/50], Step: [101/623], Train Loss: 1.4765242338180542, Val Loss: 2.793391227722168, Val Acc: 51.4555310178679\n",
      "Epoch: [3/50], Step: [201/623], Train Loss: 1.4204272031784058, Val Loss: 2.7664828300476074, Val Acc: 52.03774342501506\n",
      "Epoch: [3/50], Step: [301/623], Train Loss: 1.3693602085113525, Val Loss: 2.7409703731536865, Val Acc: 52.05781971491668\n",
      "Epoch: [3/50], Step: [401/623], Train Loss: 1.3224613666534424, Val Loss: 2.717716932296753, Val Acc: 52.720337281670346\n",
      "Epoch: [3/50], Step: [501/623], Train Loss: 1.2794259786605835, Val Loss: 2.6967594623565674, Val Acc: 53.00140534029311\n",
      "Epoch: [3/50], Step: [601/623], Train Loss: 1.2411425113677979, Val Loss: 2.678284168243408, Val Acc: 53.08171049989962\n",
      "Epoch: [3/50], Train Acc: 53.922602017768405\n",
      "Epoch: [4/50], Step: [101/623], Train Loss: 0.979540228843689, Val Loss: 2.659044027328491, Val Acc: 53.985143545472795\n",
      "Epoch: [4/50], Step: [201/623], Train Loss: 0.9593009352684021, Val Loss: 2.645808696746826, Val Acc: 54.76811885163622\n",
      "Epoch: [4/50], Step: [301/623], Train Loss: 0.941189706325531, Val Loss: 2.6343889236450195, Val Acc: 54.60750853242321\n",
      "Epoch: [4/50], Step: [401/623], Train Loss: 0.9251627326011658, Val Loss: 2.6246485710144043, Val Acc: 56.11323027504517\n",
      "Epoch: [4/50], Step: [501/623], Train Loss: 0.9100167751312256, Val Loss: 2.616204261779785, Val Acc: 56.0530014053403\n",
      "Epoch: [4/50], Step: [601/623], Train Loss: 0.8968836665153503, Val Loss: 2.608889579772949, Val Acc: 56.615137522585826\n",
      "Epoch: [4/50], Train Acc: 58.10871856648095\n",
      "Epoch: [5/50], Step: [101/623], Train Loss: 0.8067975044250488, Val Loss: 2.6011321544647217, Val Acc: 57.41818911865087\n",
      "Epoch: [5/50], Step: [201/623], Train Loss: 0.8015938401222229, Val Loss: 2.5957794189453125, Val Acc: 58.04055410560129\n",
      "Epoch: [5/50], Step: [301/623], Train Loss: 0.7954549193382263, Val Loss: 2.5911080837249756, Val Acc: 58.02047781569966\n",
      "Epoch: [5/50], Step: [401/623], Train Loss: 0.7892975807189941, Val Loss: 2.5869102478027344, Val Acc: 59.225055209797226\n",
      "Epoch: [5/50], Step: [501/623], Train Loss: 0.7838601469993591, Val Loss: 2.5832624435424805, Val Acc: 58.94398715117446\n",
      "Epoch: [5/50], Step: [601/623], Train Loss: 0.7786198854446411, Val Loss: 2.579981565475464, Val Acc: 59.626581007829756\n",
      "Epoch: [5/50], Train Acc: 61.16046780103398\n",
      "Epoch: [6/50], Step: [101/623], Train Loss: 0.7420150637626648, Val Loss: 2.576366901397705, Val Acc: 60.46978518369805\n",
      "Epoch: [6/50], Step: [201/623], Train Loss: 0.7394008636474609, Val Loss: 2.5736706256866455, Val Acc: 61.05199759084521\n",
      "Epoch: [6/50], Step: [301/623], Train Loss: 0.736605703830719, Val Loss: 2.571268081665039, Val Acc: 61.19253162015659\n",
      "Epoch: [6/50], Step: [401/623], Train Loss: 0.7339894771575928, Val Loss: 2.5690107345581055, Val Acc: 61.7345914475005\n",
      "Epoch: [6/50], Step: [501/623], Train Loss: 0.7311776280403137, Val Loss: 2.5667216777801514, Val Acc: 62.617948203172055\n",
      "Epoch: [6/50], Step: [601/623], Train Loss: 0.7286967635154724, Val Loss: 2.564626932144165, Val Acc: 63.441076089138726\n",
      "Epoch: [6/50], Train Acc: 63.89599959845405\n",
      "Epoch: [7/50], Step: [101/623], Train Loss: 0.7092440128326416, Val Loss: 2.5623228549957275, Val Acc: 63.72214414776149\n",
      "Epoch: [7/50], Step: [201/623], Train Loss: 0.70665442943573, Val Loss: 2.560535192489624, Val Acc: 64.24412768520378\n",
      "Epoch: [7/50], Step: [301/623], Train Loss: 0.7051284909248352, Val Loss: 2.5588080883026123, Val Acc: 65.04717928126882\n",
      "Epoch: [7/50], Step: [401/623], Train Loss: 0.7034671902656555, Val Loss: 2.5571653842926025, Val Acc: 65.34832362979321\n",
      "Epoch: [7/50], Step: [501/623], Train Loss: 0.7020474672317505, Val Loss: 2.5555362701416016, Val Acc: 65.60931539851435\n",
      "Epoch: [7/50], Step: [601/623], Train Loss: 0.7000820636749268, Val Loss: 2.5539021492004395, Val Acc: 66.61312989359567\n",
      "Epoch: [7/50], Train Acc: 67.7909953320283\n",
      "Epoch: [8/50], Step: [101/623], Train Loss: 0.6840304136276245, Val Loss: 2.5518646240234375, Val Acc: 67.13511343103795\n",
      "Epoch: [8/50], Step: [201/623], Train Loss: 0.6833860278129578, Val Loss: 2.5502073764801025, Val Acc: 67.09496085123469\n",
      "Epoch: [8/50], Step: [301/623], Train Loss: 0.6826871633529663, Val Loss: 2.5487616062164307, Val Acc: 68.11885163621763\n",
      "Epoch: [8/50], Step: [401/623], Train Loss: 0.6812989711761475, Val Loss: 2.547142505645752, Val Acc: 68.1790805059225\n",
      "Epoch: [8/50], Step: [501/623], Train Loss: 0.6796932220458984, Val Loss: 2.545604705810547, Val Acc: 68.92190323228267\n",
      "Epoch: [8/50], Step: [601/623], Train Loss: 0.6784307956695557, Val Loss: 2.544008731842041, Val Acc: 69.36358161011844\n",
      "Epoch: [8/50], Train Acc: 70.99834362294835\n",
      "Epoch: [9/50], Step: [101/623], Train Loss: 0.6653841137886047, Val Loss: 2.5419535636901855, Val Acc: 69.42381047982333\n",
      "Epoch: [9/50], Step: [201/623], Train Loss: 0.6650041341781616, Val Loss: 2.540433168411255, Val Acc: 70.02609917687211\n",
      "Epoch: [9/50], Step: [301/623], Train Loss: 0.6630169153213501, Val Loss: 2.538799524307251, Val Acc: 70.7889981931339\n",
      "Epoch: [9/50], Step: [401/623], Train Loss: 0.6609755754470825, Val Loss: 2.5370099544525146, Val Acc: 70.74884561333066\n",
      "Epoch: [9/50], Step: [501/623], Train Loss: 0.6594815254211426, Val Loss: 2.535383462905884, Val Acc: 71.4715920497892\n",
      "Epoch: [9/50], Step: [601/623], Train Loss: 0.6574077606201172, Val Loss: 2.5335371494293213, Val Acc: 71.7727363983136\n",
      "Epoch: [9/50], Train Acc: 73.86437785474075\n",
      "Epoch: [10/50], Step: [101/623], Train Loss: 0.6466631889343262, Val Loss: 2.5313687324523926, Val Acc: 71.8530415579201\n",
      "Epoch: [10/50], Step: [201/623], Train Loss: 0.6446760296821594, Val Loss: 2.52958345413208, Val Acc: 72.83677976309978\n",
      "Epoch: [10/50], Step: [301/623], Train Loss: 0.6427311301231384, Val Loss: 2.5278050899505615, Val Acc: 73.13792411162417\n",
      "Epoch: [10/50], Step: [401/623], Train Loss: 0.6403475999832153, Val Loss: 2.525961399078369, Val Acc: 73.19815298132905\n",
      "Epoch: [10/50], Step: [501/623], Train Loss: 0.6381564140319824, Val Loss: 2.5239412784576416, Val Acc: 73.76028909857459\n",
      "Epoch: [10/50], Step: [601/623], Train Loss: 0.6363085508346558, Val Loss: 2.5218377113342285, Val Acc: 74.32242521582012\n",
      "Epoch: [10/50], Train Acc: 76.44431059579381\n",
      "Epoch: [11/50], Step: [101/623], Train Loss: 0.6258843541145325, Val Loss: 2.5194802284240723, Val Acc: 74.86448504316402\n",
      "Epoch: [11/50], Step: [201/623], Train Loss: 0.6226285696029663, Val Loss: 2.5174753665924072, Val Acc: 75.08532423208192\n",
      "Epoch: [11/50], Step: [301/623], Train Loss: 0.6180027723312378, Val Loss: 2.5152995586395264, Val Acc: 75.3864685806063\n",
      "Epoch: [11/50], Step: [401/623], Train Loss: 0.6171149015426636, Val Loss: 2.5132462978363037, Val Acc: 75.58723147962256\n",
      "Epoch: [11/50], Step: [501/623], Train Loss: 0.6154448986053467, Val Loss: 2.5109832286834717, Val Acc: 75.80807066854045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11/50], Step: [601/623], Train Loss: 0.6132062673568726, Val Loss: 2.5087170600891113, Val Acc: 76.30997791608111\n",
      "Epoch: [11/50], Train Acc: 78.35667319178839\n",
      "Epoch: [12/50], Step: [101/623], Train Loss: 0.6014927625656128, Val Loss: 2.5060501098632812, Val Acc: 76.530817104999\n",
      "Epoch: [12/50], Step: [201/623], Train Loss: 0.5982723832130432, Val Loss: 2.5038349628448486, Val Acc: 77.11302951214616\n",
      "Epoch: [12/50], Step: [301/623], Train Loss: 0.5965666174888611, Val Loss: 2.501556873321533, Val Acc: 77.33386870106405\n",
      "Epoch: [12/50], Step: [401/623], Train Loss: 0.5936993956565857, Val Loss: 2.499162435531616, Val Acc: 77.5346316000803\n",
      "Epoch: [12/50], Step: [501/623], Train Loss: 0.5912473201751709, Val Loss: 2.4968206882476807, Val Acc: 77.79562336880144\n",
      "Epoch: [12/50], Step: [601/623], Train Loss: 0.5893024802207947, Val Loss: 2.494474411010742, Val Acc: 77.35394499096567\n",
      "Epoch: [12/50], Train Acc: 79.797219294283\n",
      "Epoch: [13/50], Step: [101/623], Train Loss: 0.5797699093818665, Val Loss: 2.491532802581787, Val Acc: 78.1369202971291\n",
      "Epoch: [13/50], Step: [201/623], Train Loss: 0.5751736164093018, Val Loss: 2.4890828132629395, Val Acc: 78.31760690624373\n",
      "Epoch: [13/50], Step: [301/623], Train Loss: 0.5708033442497253, Val Loss: 2.4866151809692383, Val Acc: 78.09676771732583\n",
      "Epoch: [13/50], Step: [401/623], Train Loss: 0.567324697971344, Val Loss: 2.4839437007904053, Val Acc: 78.79943786388276\n",
      "Epoch: [13/50], Step: [501/623], Train Loss: 0.5645577907562256, Val Loss: 2.4813106060028076, Val Acc: 78.79943786388276\n",
      "Epoch: [13/50], Step: [601/623], Train Loss: 0.5635524988174438, Val Loss: 2.478827953338623, Val Acc: 79.00020076289901\n",
      "Epoch: [13/50], Train Acc: 81.26788134317121\n",
      "Epoch: [14/50], Step: [101/623], Train Loss: 0.546035647392273, Val Loss: 2.4757697582244873, Val Acc: 79.6827946195543\n",
      "Epoch: [14/50], Step: [201/623], Train Loss: 0.5461816787719727, Val Loss: 2.4732306003570557, Val Acc: 79.2210399518169\n",
      "Epoch: [14/50], Step: [301/623], Train Loss: 0.543610155582428, Val Loss: 2.4706180095672607, Val Acc: 79.16081108211202\n",
      "Epoch: [14/50], Step: [401/623], Train Loss: 0.5425806045532227, Val Loss: 2.4681077003479004, Val Acc: 79.4418791407348\n",
      "Epoch: [14/50], Step: [501/623], Train Loss: 0.5393697023391724, Val Loss: 2.4654808044433594, Val Acc: 79.86348122866895\n",
      "Epoch: [14/50], Step: [601/623], Train Loss: 0.5377223491668701, Val Loss: 2.4629173278808594, Val Acc: 80.00401525798033\n",
      "Epoch: [14/50], Train Acc: 82.43738392812327\n",
      "Epoch: [15/50], Step: [101/623], Train Loss: 0.5192488431930542, Val Loss: 2.459707736968994, Val Acc: 80.14454928729171\n",
      "Epoch: [15/50], Step: [201/623], Train Loss: 0.5206552147865295, Val Loss: 2.457054376602173, Val Acc: 80.18470186709496\n",
      "Epoch: [15/50], Step: [301/623], Train Loss: 0.5201017260551453, Val Loss: 2.4545304775238037, Val Acc: 80.58622766512748\n",
      "Epoch: [15/50], Step: [401/623], Train Loss: 0.5171602964401245, Val Loss: 2.4520366191864014, Val Acc: 80.42561734591447\n",
      "Epoch: [15/50], Step: [501/623], Train Loss: 0.5142173171043396, Val Loss: 2.4493823051452637, Val Acc: 80.6063039550291\n",
      "Epoch: [15/50], Step: [601/623], Train Loss: 0.5120364427566528, Val Loss: 2.4466049671173096, Val Acc: 80.98775346316\n",
      "Epoch: [15/50], Train Acc: 83.34588164433067\n",
      "Epoch: [16/50], Step: [101/623], Train Loss: 0.4955897629261017, Val Loss: 2.4434189796447754, Val Acc: 81.02790604296327\n",
      "Epoch: [16/50], Step: [201/623], Train Loss: 0.4939332902431488, Val Loss: 2.4407505989074707, Val Acc: 81.04798233286489\n",
      "Epoch: [16/50], Step: [301/623], Train Loss: 0.4917871654033661, Val Loss: 2.4382426738739014, Val Acc: 81.2085926520779\n",
      "Epoch: [16/50], Step: [401/623], Train Loss: 0.48957470059394836, Val Loss: 2.4356138706207275, Val Acc: 81.28889781168441\n",
      "Epoch: [16/50], Step: [501/623], Train Loss: 0.4889882206916809, Val Loss: 2.433166027069092, Val Acc: 81.44950813089741\n",
      "Epoch: [16/50], Step: [601/623], Train Loss: 0.4868026077747345, Val Loss: 2.430745840072632, Val Acc: 81.7506524794218\n",
      "Epoch: [16/50], Train Acc: 84.17908949455403\n",
      "Epoch: [17/50], Step: [101/623], Train Loss: 0.47217729687690735, Val Loss: 2.4275383949279785, Val Acc: 81.89118650873318\n",
      "Epoch: [17/50], Step: [201/623], Train Loss: 0.46802061796188354, Val Loss: 2.4251320362091064, Val Acc: 82.01164424814294\n",
      "Epoch: [17/50], Step: [301/623], Train Loss: 0.46748265624046326, Val Loss: 2.4225151538848877, Val Acc: 82.07187311784782\n",
      "Epoch: [17/50], Step: [401/623], Train Loss: 0.4662664830684662, Val Loss: 2.420170783996582, Val Acc: 82.11202569765108\n",
      "Epoch: [17/50], Step: [501/623], Train Loss: 0.4643012583255768, Val Loss: 2.41762375831604, Val Acc: 82.45332262597871\n",
      "Epoch: [17/50], Step: [601/623], Train Loss: 0.462911993265152, Val Loss: 2.4153244495391846, Val Acc: 82.49347520578198\n",
      "Epoch: [17/50], Train Acc: 84.71113788084124\n",
      "Epoch: [18/50], Step: [101/623], Train Loss: 0.4439014792442322, Val Loss: 2.412198066711426, Val Acc: 82.6741618148966\n",
      "Epoch: [18/50], Step: [201/623], Train Loss: 0.4470500349998474, Val Loss: 2.4099514484405518, Val Acc: 82.75446697450312\n",
      "Epoch: [18/50], Step: [301/623], Train Loss: 0.4454434812068939, Val Loss: 2.4077770709991455, Val Acc: 82.83477213410961\n",
      "Epoch: [18/50], Step: [401/623], Train Loss: 0.44418978691101074, Val Loss: 2.4054787158966064, Val Acc: 82.8950010038145\n",
      "Epoch: [18/50], Step: [501/623], Train Loss: 0.4425426721572876, Val Loss: 2.40307879447937, Val Acc: 83.0556113230275\n",
      "Epoch: [18/50], Step: [601/623], Train Loss: 0.44086089730262756, Val Loss: 2.401057720184326, Val Acc: 83.07568761292913\n",
      "Epoch: [18/50], Train Acc: 85.30341815991568\n",
      "Epoch: [19/50], Step: [101/623], Train Loss: 0.427977055311203, Val Loss: 2.398183822631836, Val Acc: 83.25637422204376\n",
      "Epoch: [19/50], Step: [201/623], Train Loss: 0.42929601669311523, Val Loss: 2.396059036254883, Val Acc: 83.33667938165027\n",
      "Epoch: [19/50], Step: [301/623], Train Loss: 0.42808568477630615, Val Loss: 2.393831968307495, Val Acc: 83.3567556715519\n",
      "Epoch: [19/50], Step: [401/623], Train Loss: 0.425081729888916, Val Loss: 2.391798496246338, Val Acc: 83.2764505119454\n",
      "Epoch: [19/50], Step: [501/623], Train Loss: 0.42154544591903687, Val Loss: 2.389594554901123, Val Acc: 83.4370608311584\n",
      "Epoch: [19/50], Step: [601/623], Train Loss: 0.4211589992046356, Val Loss: 2.3876776695251465, Val Acc: 83.71812888978117\n",
      "Epoch: [19/50], Train Acc: 85.825427897405\n",
      "Epoch: [20/50], Step: [101/623], Train Loss: 0.4122849404811859, Val Loss: 2.384986400604248, Val Acc: 83.5173659907649\n",
      "Epoch: [20/50], Step: [201/623], Train Loss: 0.40504515171051025, Val Loss: 2.3830339908599854, Val Acc: 83.55751857056816\n",
      "Epoch: [20/50], Step: [301/623], Train Loss: 0.4059157371520996, Val Loss: 2.3809711933135986, Val Acc: 83.87873920899418\n",
      "Epoch: [20/50], Step: [401/623], Train Loss: 0.4052295386791229, Val Loss: 2.3790812492370605, Val Acc: 83.93896807869906\n",
      "Epoch: [20/50], Step: [501/623], Train Loss: 0.403210312128067, Val Loss: 2.3770394325256348, Val Acc: 83.99919694840393\n",
      "Epoch: [20/50], Step: [601/623], Train Loss: 0.401853084564209, Val Loss: 2.3751940727233887, Val Acc: 84.03934952820718\n",
      "Epoch: [20/50], Train Acc: 86.2721477689103\n",
      "Epoch: [21/50], Step: [101/623], Train Loss: 0.39463579654693604, Val Loss: 2.3729021549224854, Val Acc: 84.03934952820718\n",
      "Epoch: [21/50], Step: [201/623], Train Loss: 0.39035481214523315, Val Loss: 2.3709394931793213, Val Acc: 84.17988355751856\n",
      "Epoch: [21/50], Step: [301/623], Train Loss: 0.3884720802307129, Val Loss: 2.3691186904907227, Val Acc: 84.01927323830556\n",
      "Epoch: [21/50], Step: [401/623], Train Loss: 0.38795846700668335, Val Loss: 2.3674094676971436, Val Acc: 84.30034129692832\n",
      "Epoch: [21/50], Step: [501/623], Train Loss: 0.38810044527053833, Val Loss: 2.36566424369812, Val Acc: 84.34049387673159\n",
      "Epoch: [21/50], Step: [601/623], Train Loss: 0.3858724534511566, Val Loss: 2.363849639892578, Val Acc: 84.30034129692832\n",
      "Epoch: [21/50], Train Acc: 86.7188676404156\n",
      "Epoch: [22/50], Step: [101/623], Train Loss: 0.3767283856868744, Val Loss: 2.3616364002227783, Val Acc: 84.38064645653483\n",
      "Epoch: [22/50], Step: [201/623], Train Loss: 0.3734467029571533, Val Loss: 2.359966993331909, Val Acc: 84.36057016663321\n",
      "Epoch: [22/50], Step: [301/623], Train Loss: 0.37274956703186035, Val Loss: 2.3583576679229736, Val Acc: 84.42079903633808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22/50], Step: [401/623], Train Loss: 0.3732931613922119, Val Loss: 2.356921434402466, Val Acc: 84.60148564545273\n",
      "Epoch: [22/50], Step: [501/623], Train Loss: 0.3711322247982025, Val Loss: 2.3551390171051025, Val Acc: 84.58140935555109\n",
      "Epoch: [22/50], Step: [601/623], Train Loss: 0.37115058302879333, Val Loss: 2.3534843921661377, Val Acc: 84.68179080505922\n",
      "Epoch: [22/50], Train Acc: 87.19570345831451\n",
      "Epoch: [23/50], Step: [101/623], Train Loss: 0.3572979271411896, Val Loss: 2.3514978885650635, Val Acc: 84.78217225456736\n",
      "Epoch: [23/50], Step: [201/623], Train Loss: 0.3559482991695404, Val Loss: 2.350188732147217, Val Acc: 84.88255370407549\n",
      "Epoch: [23/50], Step: [301/623], Train Loss: 0.3586055040359497, Val Loss: 2.3486456871032715, Val Acc: 84.98293515358361\n",
      "Epoch: [23/50], Step: [401/623], Train Loss: 0.3588724732398987, Val Loss: 2.347038745880127, Val Acc: 85.02308773338687\n",
      "Epoch: [23/50], Step: [501/623], Train Loss: 0.3590664267539978, Val Loss: 2.3455870151519775, Val Acc: 85.02308773338687\n",
      "Epoch: [23/50], Step: [601/623], Train Loss: 0.35737308859825134, Val Loss: 2.344205617904663, Val Acc: 85.02308773338687\n",
      "Epoch: [23/50], Train Acc: 87.48180494905385\n",
      "Epoch: [24/50], Step: [101/623], Train Loss: 0.3476470708847046, Val Loss: 2.342524290084839, Val Acc: 84.98293515358361\n",
      "Epoch: [24/50], Step: [201/623], Train Loss: 0.3481657803058624, Val Loss: 2.3411667346954346, Val Acc: 85.10339289299337\n",
      "Epoch: [24/50], Step: [301/623], Train Loss: 0.3469183146953583, Val Loss: 2.3396644592285156, Val Acc: 85.24392692230475\n",
      "Epoch: [24/50], Step: [401/623], Train Loss: 0.34654223918914795, Val Loss: 2.338416337966919, Val Acc: 85.22385063240313\n",
      "Epoch: [24/50], Step: [501/623], Train Loss: 0.34616637229919434, Val Loss: 2.336963176727295, Val Acc: 85.40453724151777\n",
      "Epoch: [24/50], Step: [601/623], Train Loss: 0.34543073177337646, Val Loss: 2.3355700969696045, Val Acc: 85.40453724151777\n",
      "Epoch: [24/50], Train Acc: 87.90342819856447\n",
      "Epoch: [25/50], Step: [101/623], Train Loss: 0.33963367342948914, Val Loss: 2.3340909481048584, Val Acc: 85.44468982132102\n",
      "Epoch: [25/50], Step: [201/623], Train Loss: 0.3384445011615753, Val Loss: 2.3330540657043457, Val Acc: 85.48484240112427\n",
      "Epoch: [25/50], Step: [301/623], Train Loss: 0.3373256027698517, Val Loss: 2.3315203189849854, Val Acc: 85.5852238506324\n",
      "Epoch: [25/50], Step: [401/623], Train Loss: 0.3347957134246826, Val Loss: 2.3305773735046387, Val Acc: 85.72575787994379\n",
      "Epoch: [25/50], Step: [501/623], Train Loss: 0.3353722095489502, Val Loss: 2.3291032314300537, Val Acc: 85.72575787994379\n",
      "Epoch: [25/50], Step: [601/623], Train Loss: 0.3345244228839874, Val Loss: 2.32818865776062, Val Acc: 85.68560530014054\n",
      "Epoch: [25/50], Train Acc: 88.22466496009638\n",
      "Epoch: [26/50], Step: [101/623], Train Loss: 0.32400602102279663, Val Loss: 2.3265349864959717, Val Acc: 85.76591045974703\n",
      "Epoch: [26/50], Step: [201/623], Train Loss: 0.32409682869911194, Val Loss: 2.3252758979797363, Val Acc: 85.78598674964867\n",
      "Epoch: [26/50], Step: [301/623], Train Loss: 0.32484787702560425, Val Loss: 2.324312210083008, Val Acc: 85.82613932945192\n",
      "Epoch: [26/50], Step: [401/623], Train Loss: 0.324974924325943, Val Loss: 2.3232228755950928, Val Acc: 85.8060630395503\n",
      "Epoch: [26/50], Step: [501/623], Train Loss: 0.3252998888492584, Val Loss: 2.3221092224121094, Val Acc: 85.84621561935354\n",
      "Epoch: [26/50], Step: [601/623], Train Loss: 0.32408785820007324, Val Loss: 2.3210256099700928, Val Acc: 85.9666733587633\n",
      "Epoch: [26/50], Train Acc: 88.6211915876123\n",
      "Epoch: [27/50], Step: [101/623], Train Loss: 0.3177475035190582, Val Loss: 2.3198347091674805, Val Acc: 85.98674964866493\n",
      "Epoch: [27/50], Step: [201/623], Train Loss: 0.31600549817085266, Val Loss: 2.3186116218566895, Val Acc: 85.98674964866493\n",
      "Epoch: [27/50], Step: [301/623], Train Loss: 0.316023051738739, Val Loss: 2.317389488220215, Val Acc: 86.02690222846817\n",
      "Epoch: [27/50], Step: [401/623], Train Loss: 0.3154052197933197, Val Loss: 2.3165223598480225, Val Acc: 86.14735996787793\n",
      "Epoch: [27/50], Step: [501/623], Train Loss: 0.3144415616989136, Val Loss: 2.315547466278076, Val Acc: 86.12728367797631\n",
      "Epoch: [27/50], Step: [601/623], Train Loss: 0.3154217600822449, Val Loss: 2.3145837783813477, Val Acc: 86.12728367797631\n",
      "Epoch: [27/50], Train Acc: 88.85208050996336\n",
      "Epoch: [28/50], Step: [101/623], Train Loss: 0.30421188473701477, Val Loss: 2.3134970664978027, Val Acc: 86.22766512748444\n",
      "Epoch: [28/50], Step: [201/623], Train Loss: 0.31007879972457886, Val Loss: 2.3123326301574707, Val Acc: 86.22766512748444\n",
      "Epoch: [28/50], Step: [301/623], Train Loss: 0.3078993856906891, Val Loss: 2.3113622665405273, Val Acc: 86.26781770728769\n",
      "Epoch: [28/50], Step: [401/623], Train Loss: 0.30654093623161316, Val Loss: 2.3105409145355225, Val Acc: 86.36819915679582\n",
      "Epoch: [28/50], Step: [501/623], Train Loss: 0.3086325526237488, Val Loss: 2.309758424758911, Val Acc: 86.38827544669745\n",
      "Epoch: [28/50], Step: [601/623], Train Loss: 0.3055141270160675, Val Loss: 2.30845046043396, Val Acc: 86.46858060630396\n",
      "Epoch: [28/50], Train Acc: 89.09802740551122\n",
      "Epoch: [29/50], Step: [101/623], Train Loss: 0.2981995940208435, Val Loss: 2.307368040084839, Val Acc: 86.4284280265007\n",
      "Epoch: [29/50], Step: [201/623], Train Loss: 0.29729387164115906, Val Loss: 2.3066132068634033, Val Acc: 86.50873318610721\n",
      "Epoch: [29/50], Step: [301/623], Train Loss: 0.29808858036994934, Val Loss: 2.305694580078125, Val Acc: 86.52880947600883\n",
      "Epoch: [29/50], Step: [401/623], Train Loss: 0.2989243268966675, Val Loss: 2.3048605918884277, Val Acc: 86.58903834571372\n",
      "Epoch: [29/50], Step: [501/623], Train Loss: 0.2972011864185333, Val Loss: 2.304255723953247, Val Acc: 86.62919092551697\n",
      "Epoch: [29/50], Step: [601/623], Train Loss: 0.2964986562728882, Val Loss: 2.3032665252685547, Val Acc: 86.66934350532021\n",
      "Epoch: [29/50], Train Acc: 89.36405159865483\n",
      "Epoch: [30/50], Step: [101/623], Train Loss: 0.290280818939209, Val Loss: 2.3023815155029297, Val Acc: 86.68941979522184\n",
      "Epoch: [30/50], Step: [201/623], Train Loss: 0.2905268669128418, Val Loss: 2.3012049198150635, Val Acc: 86.68941979522184\n",
      "Epoch: [30/50], Step: [301/623], Train Loss: 0.2911979854106903, Val Loss: 2.300633668899536, Val Acc: 86.70949608512348\n",
      "Epoch: [30/50], Step: [401/623], Train Loss: 0.289696604013443, Val Loss: 2.299752950668335, Val Acc: 86.64926721541859\n",
      "Epoch: [30/50], Step: [501/623], Train Loss: 0.2888740301132202, Val Loss: 2.2988295555114746, Val Acc: 86.70949608512348\n",
      "Epoch: [30/50], Step: [601/623], Train Loss: 0.29037022590637207, Val Loss: 2.2982232570648193, Val Acc: 86.85003011443486\n",
      "Epoch: [30/50], Train Acc: 89.66019173819204\n",
      "Epoch: [31/50], Step: [101/623], Train Loss: 0.29089078307151794, Val Loss: 2.297327756881714, Val Acc: 86.78980124472997\n",
      "Epoch: [31/50], Step: [201/623], Train Loss: 0.28806445002555847, Val Loss: 2.296557664871216, Val Acc: 86.8901826942381\n",
      "Epoch: [31/50], Step: [301/623], Train Loss: 0.28572869300842285, Val Loss: 2.295724391937256, Val Acc: 86.85003011443486\n",
      "Epoch: [31/50], Step: [401/623], Train Loss: 0.28318819403648376, Val Loss: 2.295093536376953, Val Acc: 86.95041156394298\n",
      "Epoch: [31/50], Step: [501/623], Train Loss: 0.2828790843486786, Val Loss: 2.29418683052063, Val Acc: 86.87010640433648\n",
      "Epoch: [31/50], Step: [601/623], Train Loss: 0.2832605838775635, Val Loss: 2.2934699058532715, Val Acc: 86.8901826942381\n",
      "Epoch: [31/50], Train Acc: 89.95633187772926\n",
      "Epoch: [32/50], Step: [101/623], Train Loss: 0.27150580286979675, Val Loss: 2.2926549911499023, Val Acc: 86.9704878538446\n",
      "Epoch: [32/50], Step: [201/623], Train Loss: 0.27411335706710815, Val Loss: 2.292228937149048, Val Acc: 86.93033527404135\n",
      "Epoch: [32/50], Step: [301/623], Train Loss: 0.2760658264160156, Val Loss: 2.2913711071014404, Val Acc: 86.99056414374624\n",
      "Epoch: [32/50], Step: [401/623], Train Loss: 0.2762964069843292, Val Loss: 2.290736675262451, Val Acc: 87.07086930335274\n",
      "Epoch: [32/50], Step: [501/623], Train Loss: 0.2744559049606323, Val Loss: 2.2898712158203125, Val Acc: 87.03071672354949\n",
      "Epoch: [32/50], Step: [601/623], Train Loss: 0.277008593082428, Val Loss: 2.289199113845825, Val Acc: 87.03071672354949\n",
      "Epoch: [32/50], Train Acc: 90.24745269286754\n",
      "Epoch: [33/50], Step: [101/623], Train Loss: 0.2698936462402344, Val Loss: 2.28844952583313, Val Acc: 87.09094559325436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33/50], Step: [201/623], Train Loss: 0.2657370865345001, Val Loss: 2.28775954246521, Val Acc: 87.17125075286087\n",
      "Epoch: [33/50], Step: [301/623], Train Loss: 0.2713002860546112, Val Loss: 2.2875092029571533, Val Acc: 87.13109817305762\n",
      "Epoch: [33/50], Step: [401/623], Train Loss: 0.27038782835006714, Val Loss: 2.2868053913116455, Val Acc: 87.21140333266412\n",
      "Epoch: [33/50], Step: [501/623], Train Loss: 0.271836519241333, Val Loss: 2.286101818084717, Val Acc: 87.17125075286087\n",
      "Epoch: [33/50], Step: [601/623], Train Loss: 0.2712031900882721, Val Loss: 2.2857203483581543, Val Acc: 87.25155591246738\n",
      "Epoch: [33/50], Train Acc: 90.44822566882497\n",
      "Epoch: [34/50], Step: [101/623], Train Loss: 0.2605854272842407, Val Loss: 2.284736394882202, Val Acc: 87.23147962256574\n",
      "Epoch: [34/50], Step: [201/623], Train Loss: 0.2633645832538605, Val Loss: 2.2841033935546875, Val Acc: 87.25155591246738\n",
      "Epoch: [34/50], Step: [301/623], Train Loss: 0.26120492815971375, Val Loss: 2.28340220451355, Val Acc: 87.33186107207388\n",
      "Epoch: [34/50], Step: [401/623], Train Loss: 0.2611196041107178, Val Loss: 2.282750368118286, Val Acc: 87.3519373619755\n",
      "Epoch: [34/50], Step: [501/623], Train Loss: 0.2628152370452881, Val Loss: 2.282153844833374, Val Acc: 87.31178478217225\n",
      "Epoch: [34/50], Step: [601/623], Train Loss: 0.264256089925766, Val Loss: 2.281975269317627, Val Acc: 87.47239510138526\n",
      "Epoch: [34/50], Train Acc: 90.66405661797921\n",
      "Epoch: [35/50], Step: [101/623], Train Loss: 0.26209747791290283, Val Loss: 2.2812695503234863, Val Acc: 87.45231881148364\n",
      "Epoch: [35/50], Step: [201/623], Train Loss: 0.26046043634414673, Val Loss: 2.2805116176605225, Val Acc: 87.51254768118852\n",
      "Epoch: [35/50], Step: [301/623], Train Loss: 0.2597315311431885, Val Loss: 2.280062437057495, Val Acc: 87.45231881148364\n",
      "Epoch: [35/50], Step: [401/623], Train Loss: 0.26184651255607605, Val Loss: 2.2795474529266357, Val Acc: 87.41216623168039\n",
      "Epoch: [35/50], Step: [501/623], Train Loss: 0.260352224111557, Val Loss: 2.2787468433380127, Val Acc: 87.61292913069664\n",
      "Epoch: [35/50], Step: [601/623], Train Loss: 0.257968544960022, Val Loss: 2.2783522605895996, Val Acc: 87.55270026099177\n",
      "Epoch: [35/50], Train Acc: 90.89494554033027\n",
      "Epoch: [36/50], Step: [101/623], Train Loss: 0.2597942352294922, Val Loss: 2.277744770050049, Val Acc: 87.59285284079502\n",
      "Epoch: [36/50], Step: [201/623], Train Loss: 0.2516769766807556, Val Loss: 2.277236223220825, Val Acc: 87.7333868701064\n",
      "Epoch: [36/50], Step: [301/623], Train Loss: 0.25439387559890747, Val Loss: 2.276759624481201, Val Acc: 87.7333868701064\n",
      "Epoch: [36/50], Step: [401/623], Train Loss: 0.2536689341068268, Val Loss: 2.276181221008301, Val Acc: 87.71331058020478\n",
      "Epoch: [36/50], Step: [501/623], Train Loss: 0.25486883521080017, Val Loss: 2.2757885456085205, Val Acc: 87.87392089941778\n",
      "Epoch: [36/50], Step: [601/623], Train Loss: 0.2538825273513794, Val Loss: 2.275151491165161, Val Acc: 87.69323429030315\n",
      "Epoch: [36/50], Train Acc: 91.08567986748983\n",
      "Epoch: [37/50], Step: [101/623], Train Loss: 0.24093684554100037, Val Loss: 2.2747135162353516, Val Acc: 87.77353944990966\n",
      "Epoch: [37/50], Step: [201/623], Train Loss: 0.2511426508426666, Val Loss: 2.2742319107055664, Val Acc: 87.87392089941778\n",
      "Epoch: [37/50], Step: [301/623], Train Loss: 0.24653422832489014, Val Loss: 2.273822784423828, Val Acc: 87.75346316000802\n",
      "Epoch: [37/50], Step: [401/623], Train Loss: 0.2479291409254074, Val Loss: 2.2731499671936035, Val Acc: 87.83376831961453\n",
      "Epoch: [37/50], Step: [501/623], Train Loss: 0.2486654818058014, Val Loss: 2.272651195526123, Val Acc: 87.93414976912267\n",
      "Epoch: [37/50], Step: [601/623], Train Loss: 0.24867407977581024, Val Loss: 2.2722973823547363, Val Acc: 87.87392089941778\n",
      "Epoch: [37/50], Train Acc: 91.29147216784621\n",
      "Epoch: [38/50], Step: [101/623], Train Loss: 0.24753296375274658, Val Loss: 2.2717957496643066, Val Acc: 88.01445492872917\n",
      "Epoch: [38/50], Step: [201/623], Train Loss: 0.2531815767288208, Val Loss: 2.27131986618042, Val Acc: 87.93414976912267\n",
      "Epoch: [38/50], Step: [301/623], Train Loss: 0.24853655695915222, Val Loss: 2.2709856033325195, Val Acc: 87.99437863882754\n",
      "Epoch: [38/50], Step: [401/623], Train Loss: 0.24685034155845642, Val Loss: 2.270538330078125, Val Acc: 87.9542260590243\n",
      "Epoch: [38/50], Step: [501/623], Train Loss: 0.244842991232872, Val Loss: 2.2699878215789795, Val Acc: 87.97430234892592\n",
      "Epoch: [38/50], Step: [601/623], Train Loss: 0.24392718076705933, Val Loss: 2.269603729248047, Val Acc: 88.07468379843405\n",
      "Epoch: [38/50], Train Acc: 91.44205189981429\n",
      "Epoch: [39/50], Step: [101/623], Train Loss: 0.24228490889072418, Val Loss: 2.2690348625183105, Val Acc: 88.0345312186308\n",
      "Epoch: [39/50], Step: [201/623], Train Loss: 0.243301659822464, Val Loss: 2.268625020980835, Val Acc: 88.01445492872917\n",
      "Epoch: [39/50], Step: [301/623], Train Loss: 0.24100978672504425, Val Loss: 2.2682085037231445, Val Acc: 88.07468379843405\n",
      "Epoch: [39/50], Step: [401/623], Train Loss: 0.2414141744375229, Val Loss: 2.2677299976348877, Val Acc: 88.1148363782373\n",
      "Epoch: [39/50], Step: [501/623], Train Loss: 0.23902583122253418, Val Loss: 2.2672431468963623, Val Acc: 88.15498895804055\n",
      "Epoch: [39/50], Step: [601/623], Train Loss: 0.2388485223054886, Val Loss: 2.2671446800231934, Val Acc: 88.21521782774543\n",
      "Epoch: [39/50], Train Acc: 91.63780555137278\n",
      "Epoch: [40/50], Step: [101/623], Train Loss: 0.22786979377269745, Val Loss: 2.2662854194641113, Val Acc: 88.15498895804055\n",
      "Epoch: [40/50], Step: [201/623], Train Loss: 0.23030298948287964, Val Loss: 2.26595139503479, Val Acc: 88.19514153784381\n",
      "Epoch: [40/50], Step: [301/623], Train Loss: 0.2364538013935089, Val Loss: 2.2656445503234863, Val Acc: 88.17506524794219\n",
      "Epoch: [40/50], Step: [401/623], Train Loss: 0.2341235727071762, Val Loss: 2.2651710510253906, Val Acc: 88.17506524794219\n",
      "Epoch: [40/50], Step: [501/623], Train Loss: 0.23436155915260315, Val Loss: 2.2647480964660645, Val Acc: 88.23529411764706\n",
      "Epoch: [40/50], Step: [601/623], Train Loss: 0.23405690491199493, Val Loss: 2.264594793319702, Val Acc: 88.2754466974503\n",
      "Epoch: [40/50], Train Acc: 91.81348190533554\n",
      "Epoch: [41/50], Step: [101/623], Train Loss: 0.22798381745815277, Val Loss: 2.2641382217407227, Val Acc: 88.33567556715519\n",
      "Epoch: [41/50], Step: [201/623], Train Loss: 0.22918131947517395, Val Loss: 2.263549327850342, Val Acc: 88.33567556715519\n",
      "Epoch: [41/50], Step: [301/623], Train Loss: 0.22914369404315948, Val Loss: 2.2632062435150146, Val Acc: 88.2754466974503\n",
      "Epoch: [41/50], Step: [401/623], Train Loss: 0.2296828329563141, Val Loss: 2.262805223464966, Val Acc: 88.25537040754868\n",
      "Epoch: [41/50], Step: [501/623], Train Loss: 0.23000214993953705, Val Loss: 2.262617349624634, Val Acc: 88.33567556715519\n",
      "Epoch: [41/50], Step: [601/623], Train Loss: 0.23020678758621216, Val Loss: 2.2618725299835205, Val Acc: 88.4962858863682\n",
      "Epoch: [41/50], Train Acc: 91.94900366410681\n",
      "Epoch: [42/50], Step: [101/623], Train Loss: 0.23504990339279175, Val Loss: 2.2616920471191406, Val Acc: 88.25537040754868\n",
      "Epoch: [42/50], Step: [201/623], Train Loss: 0.22724877297878265, Val Loss: 2.261167049407959, Val Acc: 88.29552298735194\n",
      "Epoch: [42/50], Step: [301/623], Train Loss: 0.22772781550884247, Val Loss: 2.2608656883239746, Val Acc: 88.45613330656495\n",
      "Epoch: [42/50], Step: [401/623], Train Loss: 0.2286108136177063, Val Loss: 2.2606091499328613, Val Acc: 88.43605701666333\n",
      "Epoch: [42/50], Step: [501/623], Train Loss: 0.22712145745754242, Val Loss: 2.2604126930236816, Val Acc: 88.45613330656495\n",
      "Epoch: [42/50], Step: [601/623], Train Loss: 0.22600635886192322, Val Loss: 2.2600293159484863, Val Acc: 88.41598072676169\n",
      "Epoch: [42/50], Train Acc: 92.13471866686744\n",
      "Epoch: [43/50], Step: [101/623], Train Loss: 0.22320139408111572, Val Loss: 2.2596068382263184, Val Acc: 88.39590443686006\n",
      "Epoch: [43/50], Step: [201/623], Train Loss: 0.22240275144577026, Val Loss: 2.25921893119812, Val Acc: 88.4962858863682\n",
      "Epoch: [43/50], Step: [301/623], Train Loss: 0.22065073251724243, Val Loss: 2.2585911750793457, Val Acc: 88.45613330656495\n",
      "Epoch: [43/50], Step: [401/623], Train Loss: 0.22275325655937195, Val Loss: 2.258291006088257, Val Acc: 88.47620959646657\n",
      "Epoch: [43/50], Step: [501/623], Train Loss: 0.2219974547624588, Val Loss: 2.2581019401550293, Val Acc: 88.53643846617145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [43/50], Step: [601/623], Train Loss: 0.22240012884140015, Val Loss: 2.257930278778076, Val Acc: 88.4962858863682\n",
      "Epoch: [43/50], Train Acc: 92.29031772323445\n",
      "Epoch: [44/50], Step: [101/623], Train Loss: 0.217281773686409, Val Loss: 2.2573163509368896, Val Acc: 88.55651475607307\n",
      "Epoch: [44/50], Step: [201/623], Train Loss: 0.21584700047969818, Val Loss: 2.2571187019348145, Val Acc: 88.57659104597471\n",
      "Epoch: [44/50], Step: [301/623], Train Loss: 0.21835611760616302, Val Loss: 2.2565388679504395, Val Acc: 88.57659104597471\n",
      "Epoch: [44/50], Step: [401/623], Train Loss: 0.2160627841949463, Val Loss: 2.2562177181243896, Val Acc: 88.55651475607307\n",
      "Epoch: [44/50], Step: [501/623], Train Loss: 0.2168601155281067, Val Loss: 2.256201982498169, Val Acc: 88.59666733587633\n",
      "Epoch: [44/50], Step: [601/623], Train Loss: 0.21733862161636353, Val Loss: 2.2558276653289795, Val Acc: 88.57659104597471\n",
      "Epoch: [44/50], Train Acc: 92.46097475279828\n",
      "Epoch: [45/50], Step: [101/623], Train Loss: 0.21583211421966553, Val Loss: 2.25527024269104, Val Acc: 88.61674362577796\n",
      "Epoch: [45/50], Step: [201/623], Train Loss: 0.21504127979278564, Val Loss: 2.2549736499786377, Val Acc: 88.4962858863682\n",
      "Epoch: [45/50], Step: [301/623], Train Loss: 0.21510471403598785, Val Loss: 2.2548255920410156, Val Acc: 88.67697249548283\n",
      "Epoch: [45/50], Step: [401/623], Train Loss: 0.21184423565864563, Val Loss: 2.254373788833618, Val Acc: 88.53643846617145\n",
      "Epoch: [45/50], Step: [501/623], Train Loss: 0.21351458132266998, Val Loss: 2.2539825439453125, Val Acc: 88.63681991567958\n",
      "Epoch: [45/50], Step: [601/623], Train Loss: 0.21348464488983154, Val Loss: 2.2539100646972656, Val Acc: 88.6568962055812\n",
      "Epoch: [45/50], Train Acc: 92.62159313356422\n",
      "Epoch: [46/50], Step: [101/623], Train Loss: 0.19480818510055542, Val Loss: 2.253532648086548, Val Acc: 88.73720136518772\n",
      "Epoch: [46/50], Step: [201/623], Train Loss: 0.2026880383491516, Val Loss: 2.2532832622528076, Val Acc: 88.75727765508934\n",
      "Epoch: [46/50], Step: [301/623], Train Loss: 0.2069433182477951, Val Loss: 2.252908945083618, Val Acc: 88.63681991567958\n",
      "Epoch: [46/50], Step: [401/623], Train Loss: 0.20892758667469025, Val Loss: 2.2526893615722656, Val Acc: 88.69704878538447\n",
      "Epoch: [46/50], Step: [501/623], Train Loss: 0.20930689573287964, Val Loss: 2.252373695373535, Val Acc: 88.6568962055812\n",
      "Epoch: [46/50], Step: [601/623], Train Loss: 0.20965412259101868, Val Loss: 2.251878261566162, Val Acc: 88.63681991567958\n",
      "Epoch: [46/50], Train Acc: 92.74205691913869\n",
      "Epoch: [47/50], Step: [101/623], Train Loss: 0.20143362879753113, Val Loss: 2.2515885829925537, Val Acc: 88.73720136518772\n",
      "Epoch: [47/50], Step: [201/623], Train Loss: 0.20329299569129944, Val Loss: 2.251378059387207, Val Acc: 88.69704878538447\n",
      "Epoch: [47/50], Step: [301/623], Train Loss: 0.20248781144618988, Val Loss: 2.2511754035949707, Val Acc: 88.73720136518772\n",
      "Epoch: [47/50], Step: [401/623], Train Loss: 0.20633189380168915, Val Loss: 2.250715970993042, Val Acc: 88.79743023489259\n",
      "Epoch: [47/50], Step: [501/623], Train Loss: 0.2052621692419052, Val Loss: 2.2505993843078613, Val Acc: 88.73720136518772\n",
      "Epoch: [47/50], Step: [601/623], Train Loss: 0.2061213105916977, Val Loss: 2.250383138656616, Val Acc: 88.73720136518772\n",
      "Epoch: [47/50], Train Acc: 92.93279124629825\n",
      "Epoch: [48/50], Step: [101/623], Train Loss: 0.1998392939567566, Val Loss: 2.2499940395355225, Val Acc: 88.73720136518772\n",
      "Epoch: [48/50], Step: [201/623], Train Loss: 0.20908679068088531, Val Loss: 2.249494791030884, Val Acc: 88.73720136518772\n",
      "Epoch: [48/50], Step: [301/623], Train Loss: 0.20635996758937836, Val Loss: 2.249791145324707, Val Acc: 88.93796426420397\n",
      "Epoch: [48/50], Step: [401/623], Train Loss: 0.20408959686756134, Val Loss: 2.249070405960083, Val Acc: 88.75727765508934\n",
      "Epoch: [48/50], Step: [501/623], Train Loss: 0.20320570468902588, Val Loss: 2.248854875564575, Val Acc: 88.81750652479421\n",
      "Epoch: [48/50], Step: [601/623], Train Loss: 0.20219686627388, Val Loss: 2.2486417293548584, Val Acc: 88.79743023489259\n",
      "Epoch: [48/50], Train Acc: 93.06831300506951\n",
      "Epoch: [49/50], Step: [101/623], Train Loss: 0.20607514679431915, Val Loss: 2.248392343521118, Val Acc: 88.91788797430235\n",
      "Epoch: [49/50], Step: [201/623], Train Loss: 0.19809404015541077, Val Loss: 2.248131275177002, Val Acc: 88.91788797430235\n",
      "Epoch: [49/50], Step: [301/623], Train Loss: 0.19936035573482513, Val Loss: 2.2477192878723145, Val Acc: 88.89781168440072\n",
      "Epoch: [49/50], Step: [401/623], Train Loss: 0.19887475669384003, Val Loss: 2.2475106716156006, Val Acc: 88.79743023489259\n",
      "Epoch: [49/50], Step: [501/623], Train Loss: 0.1989031881093979, Val Loss: 2.247443675994873, Val Acc: 88.85765910459747\n",
      "Epoch: [49/50], Step: [601/623], Train Loss: 0.1991192251443863, Val Loss: 2.247020721435547, Val Acc: 88.8777353944991\n",
      "Epoch: [49/50], Train Acc: 93.24398935903227\n",
      "Epoch: [50/50], Step: [101/623], Train Loss: 0.20459802448749542, Val Loss: 2.246818780899048, Val Acc: 88.91788797430235\n",
      "Epoch: [50/50], Step: [201/623], Train Loss: 0.196017786860466, Val Loss: 2.2466206550598145, Val Acc: 88.93796426420397\n",
      "Training stopped at epoch 50, iteration 201\n",
      "Learning rate: 0.0001 - Best Val Acc: 88.93796426420397\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 50\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 50 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_lr0001 = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_lr0001.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "\n",
    "print('Learning rate: 0.0001 - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. annealing from 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/20], Step: [101/623], Learning Rate: 0.01, Train Loss: 0.9195836782455444, Val Loss: 2.4403533935546875, Val Acc: 76.9925717727364\n",
      "Epoch: [1/20], Step: [201/623], Learning Rate: 0.01, Train Loss: 0.6639795303344727, Val Loss: 2.2982096672058105, Val Acc: 86.64926721541859\n",
      "Epoch: [1/20], Step: [301/623], Learning Rate: 0.01, Train Loss: 0.5570310354232788, Val Loss: 2.265956401824951, Val Acc: 88.09476008833568\n",
      "Epoch: [1/20], Step: [401/623], Learning Rate: 0.01, Train Loss: 0.49263182282447815, Val Loss: 2.248666524887085, Val Acc: 89.05842200361373\n",
      "Epoch: [1/20], Step: [501/623], Learning Rate: 0.01, Train Loss: 0.4499059021472931, Val Loss: 2.239427328109741, Val Acc: 89.07849829351535\n",
      "Epoch: [1/20], Step: [601/623], Learning Rate: 0.01, Train Loss: 0.4208346903324127, Val Loss: 2.231264114379883, Val Acc: 89.27926119253162\n",
      "Epoch: [1/20], Train Acc: 94.95557897906941\n",
      "Epoch: [2/20], Step: [101/623], Learning Rate: 0.001, Train Loss: 0.15343880653381348, Val Loss: 2.2207820415496826, Val Acc: 89.6406344107609\n",
      "Epoch: [2/20], Step: [201/623], Learning Rate: 0.001, Train Loss: 0.15231764316558838, Val Loss: 2.2166383266448975, Val Acc: 89.62055812085927\n",
      "Epoch: [2/20], Step: [301/623], Learning Rate: 0.001, Train Loss: 0.1581748127937317, Val Loss: 2.216482639312744, Val Acc: 89.37964264203976\n",
      "Epoch: [2/20], Step: [401/623], Learning Rate: 0.001, Train Loss: 0.1601826697587967, Val Loss: 2.21688175201416, Val Acc: 89.27926119253162\n",
      "Epoch: [2/20], Step: [501/623], Learning Rate: 0.001, Train Loss: 0.16087262332439423, Val Loss: 2.2132365703582764, Val Acc: 89.3394900622365\n",
      "Epoch: [2/20], Step: [601/623], Learning Rate: 0.001, Train Loss: 0.16587038338184357, Val Loss: 2.2155189514160156, Val Acc: 89.27926119253162\n",
      "Epoch: [2/20], Train Acc: 98.21312051397882\n",
      "Epoch: [3/20], Step: [101/623], Learning Rate: 0.0001, Train Loss: 0.08197114616632462, Val Loss: 2.211456775665283, Val Acc: 89.09857458341699\n",
      "Epoch: [3/20], Step: [201/623], Learning Rate: 0.0001, Train Loss: 0.07871872931718826, Val Loss: 2.2089731693267822, Val Acc: 88.97811684400723\n",
      "Epoch: [3/20], Step: [301/623], Learning Rate: 0.0001, Train Loss: 0.07778894901275635, Val Loss: 2.2105822563171387, Val Acc: 88.71712507528609\n",
      "Epoch: [3/20], Step: [401/623], Learning Rate: 0.0001, Train Loss: 0.0794316828250885, Val Loss: 2.2099523544311523, Val Acc: 88.43605701666333\n",
      "Epoch: [3/20], Step: [501/623], Learning Rate: 0.0001, Train Loss: 0.08277983218431473, Val Loss: 2.208980083465576, Val Acc: 88.35575185705682\n",
      "Epoch: [3/20], Step: [601/623], Learning Rate: 0.0001, Train Loss: 0.08397072553634644, Val Loss: 2.2109785079956055, Val Acc: 88.09476008833568\n",
      "Training stopped at epoch 3, iteration 601\n",
      "Learning rate: annealing from 0.01 - Best Val Acc: 89.6406344107609\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 50\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_anneal = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_anneal.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Learning Rate: {}, Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), learning_rate, np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "#                 if bad_iter >= 3:\n",
    "#                     learning_rate /= 10\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "    learning_rate /= 10\n",
    "\n",
    "print('Learning rate: annealing from 0.01 - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. annealing from 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/20], Step: [101/623], Learning Rate: 0.001, Train Loss: 2.5686779022216797, Val Loss: 2.9058191776275635, Val Acc: 54.14575386468581\n",
      "Epoch: [1/20], Step: [201/623], Learning Rate: 0.001, Train Loss: 1.9776729345321655, Val Loss: 2.6565258502960205, Val Acc: 58.22124071471592\n",
      "Epoch: [1/20], Step: [301/623], Learning Rate: 0.001, Train Loss: 1.594007134437561, Val Loss: 2.5815064907073975, Val Acc: 59.9678779361574\n",
      "Epoch: [1/20], Step: [401/623], Learning Rate: 0.001, Train Loss: 1.3770161867141724, Val Loss: 2.5591492652893066, Val Acc: 67.23549488054607\n",
      "Epoch: [1/20], Step: [501/623], Learning Rate: 0.001, Train Loss: 1.2391889095306396, Val Loss: 2.543131113052368, Val Acc: 68.11885163621763\n",
      "Epoch: [1/20], Step: [601/623], Learning Rate: 0.001, Train Loss: 1.1419308185577393, Val Loss: 2.525670289993286, Val Acc: 73.88074683798435\n",
      "Epoch: [1/20], Train Acc: 77.10686141645334\n",
      "Epoch: [2/20], Step: [101/623], Learning Rate: 0.0001, Train Loss: 0.6032608151435852, Val Loss: 2.498476505279541, Val Acc: 78.57859867496487\n",
      "Epoch: [2/20], Step: [201/623], Learning Rate: 0.0001, Train Loss: 0.5826349854469299, Val Loss: 2.4722259044647217, Val Acc: 80.20477815699658\n",
      "Epoch: [2/20], Step: [301/623], Learning Rate: 0.0001, Train Loss: 0.5635882616043091, Val Loss: 2.4461610317230225, Val Acc: 81.77072876932343\n",
      "Epoch: [2/20], Step: [401/623], Learning Rate: 0.0001, Train Loss: 0.546820342540741, Val Loss: 2.4214422702789307, Val Acc: 83.03553503312588\n",
      "Epoch: [2/20], Step: [501/623], Learning Rate: 0.0001, Train Loss: 0.5289035439491272, Val Loss: 2.3988630771636963, Val Acc: 84.09957839791207\n",
      "Epoch: [2/20], Step: [601/623], Learning Rate: 0.0001, Train Loss: 0.5121907591819763, Val Loss: 2.37874436378479, Val Acc: 84.86247741417387\n",
      "Epoch: [2/20], Train Acc: 86.69377101842092\n",
      "Epoch: [3/20], Step: [101/623], Learning Rate: 1e-05, Train Loss: 0.3857327699661255, Val Loss: 2.357882261276245, Val Acc: 85.44468982132102\n",
      "Epoch: [3/20], Step: [201/623], Learning Rate: 1e-05, Train Loss: 0.37997695803642273, Val Loss: 2.343860387802124, Val Acc: 86.16743625777956\n",
      "Epoch: [3/20], Step: [301/623], Learning Rate: 1e-05, Train Loss: 0.36617234349250793, Val Loss: 2.330457925796509, Val Acc: 86.40835173659907\n",
      "Epoch: [3/20], Step: [401/623], Learning Rate: 1e-05, Train Loss: 0.3590470552444458, Val Loss: 2.3197600841522217, Val Acc: 86.62919092551697\n",
      "Epoch: [3/20], Step: [501/623], Learning Rate: 1e-05, Train Loss: 0.3513382077217102, Val Loss: 2.3098888397216797, Val Acc: 87.13109817305762\n",
      "Epoch: [3/20], Step: [601/623], Learning Rate: 1e-05, Train Loss: 0.34396490454673767, Val Loss: 2.3015763759613037, Val Acc: 87.29170849227063\n",
      "Epoch: [3/20], Train Acc: 89.68026903578779\n",
      "Epoch: [4/20], Step: [101/623], Learning Rate: 1.0000000000000002e-06, Train Loss: 0.2786612808704376, Val Loss: 2.292921543121338, Val Acc: 87.61292913069664\n",
      "Epoch: [4/20], Step: [201/623], Learning Rate: 1.0000000000000002e-06, Train Loss: 0.2760172486305237, Val Loss: 2.286529064178467, Val Acc: 87.51254768118852\n",
      "Epoch: [4/20], Step: [301/623], Learning Rate: 1.0000000000000002e-06, Train Loss: 0.2726585865020752, Val Loss: 2.279945135116577, Val Acc: 88.05460750853243\n",
      "Epoch: [4/20], Step: [401/623], Learning Rate: 1.0000000000000002e-06, Train Loss: 0.27014678716659546, Val Loss: 2.2753307819366455, Val Acc: 88.2754466974503\n",
      "Epoch: [4/20], Step: [501/623], Learning Rate: 1.0000000000000002e-06, Train Loss: 0.26839685440063477, Val Loss: 2.2707316875457764, Val Acc: 88.13491266813892\n",
      "Epoch: [4/20], Step: [601/623], Learning Rate: 1.0000000000000002e-06, Train Loss: 0.26674923300743103, Val Loss: 2.267439126968384, Val Acc: 88.2754466974503\n",
      "Epoch: [4/20], Train Acc: 92.01425488129298\n",
      "Epoch: [5/20], Step: [101/623], Learning Rate: 1.0000000000000002e-07, Train Loss: 0.23676928877830505, Val Loss: 2.2629332542419434, Val Acc: 88.43605701666333\n",
      "Epoch: [5/20], Step: [201/623], Learning Rate: 1.0000000000000002e-07, Train Loss: 0.23319895565509796, Val Loss: 2.259343147277832, Val Acc: 88.59666733587633\n",
      "Epoch: [5/20], Step: [301/623], Learning Rate: 1.0000000000000002e-07, Train Loss: 0.22735002636909485, Val Loss: 2.256347417831421, Val Acc: 88.61674362577796\n",
      "Epoch: [5/20], Step: [401/623], Learning Rate: 1.0000000000000002e-07, Train Loss: 0.22745123505592346, Val Loss: 2.2531542778015137, Val Acc: 88.77735394499096\n",
      "Epoch: [5/20], Step: [501/623], Learning Rate: 1.0000000000000002e-07, Train Loss: 0.22413842380046844, Val Loss: 2.2501959800720215, Val Acc: 88.93796426420397\n",
      "Epoch: [5/20], Step: [601/623], Learning Rate: 1.0000000000000002e-07, Train Loss: 0.22166572511196136, Val Loss: 2.2476069927215576, Val Acc: 88.99819313390886\n",
      "Epoch: [5/20], Train Acc: 93.4447623349897\n",
      "Epoch: [6/20], Step: [101/623], Learning Rate: 1.0000000000000002e-08, Train Loss: 0.18998490273952484, Val Loss: 2.2448766231536865, Val Acc: 88.95804055410561\n",
      "Epoch: [6/20], Step: [201/623], Learning Rate: 1.0000000000000002e-08, Train Loss: 0.19064122438430786, Val Loss: 2.2427546977996826, Val Acc: 89.07849829351535\n",
      "Epoch: [6/20], Step: [301/623], Learning Rate: 1.0000000000000002e-08, Train Loss: 0.18900635838508606, Val Loss: 2.241002321243286, Val Acc: 89.13872716322024\n",
      "Epoch: [6/20], Step: [401/623], Learning Rate: 1.0000000000000002e-08, Train Loss: 0.18803226947784424, Val Loss: 2.238300085067749, Val Acc: 89.23910861272837\n",
      "Epoch: [6/20], Step: [501/623], Learning Rate: 1.0000000000000002e-08, Train Loss: 0.1871708780527115, Val Loss: 2.2376327514648438, Val Acc: 89.19895603292511\n",
      "Epoch: [6/20], Step: [601/623], Learning Rate: 1.0000000000000002e-08, Train Loss: 0.18788187205791473, Val Loss: 2.2349448204040527, Val Acc: 89.419795221843\n",
      "Epoch: [6/20], Train Acc: 94.79496059830346\n",
      "Epoch: [7/20], Step: [101/623], Learning Rate: 1.0000000000000003e-09, Train Loss: 0.16704653203487396, Val Loss: 2.233898878097534, Val Acc: 89.35956635213813\n",
      "Epoch: [7/20], Step: [201/623], Learning Rate: 1.0000000000000003e-09, Train Loss: 0.1680753082036972, Val Loss: 2.2320289611816406, Val Acc: 89.48002409154788\n",
      "Epoch: [7/20], Step: [301/623], Learning Rate: 1.0000000000000003e-09, Train Loss: 0.1634605973958969, Val Loss: 2.2303221225738525, Val Acc: 89.50010038144951\n",
      "Epoch: [7/20], Step: [401/623], Learning Rate: 1.0000000000000003e-09, Train Loss: 0.16390295326709747, Val Loss: 2.2290890216827393, Val Acc: 89.419795221843\n",
      "Epoch: [7/20], Step: [501/623], Learning Rate: 1.0000000000000003e-09, Train Loss: 0.1626156121492386, Val Loss: 2.2281486988067627, Val Acc: 89.6406344107609\n",
      "Epoch: [7/20], Step: [601/623], Learning Rate: 1.0000000000000003e-09, Train Loss: 0.16112877428531647, Val Loss: 2.2268433570861816, Val Acc: 89.6406344107609\n",
      "Epoch: [7/20], Train Acc: 95.72353561210662\n",
      "Epoch: [8/20], Step: [101/623], Learning Rate: 1.0000000000000003e-10, Train Loss: 0.13885964453220367, Val Loss: 2.225346803665161, Val Acc: 89.86147359967877\n",
      "Epoch: [8/20], Step: [201/623], Learning Rate: 1.0000000000000003e-10, Train Loss: 0.13742099702358246, Val Loss: 2.224606513977051, Val Acc: 89.98193133908853\n",
      "Epoch: [8/20], Step: [301/623], Learning Rate: 1.0000000000000003e-10, Train Loss: 0.13956478238105774, Val Loss: 2.2240991592407227, Val Acc: 89.74101586026902\n",
      "Epoch: [8/20], Step: [401/623], Learning Rate: 1.0000000000000003e-10, Train Loss: 0.1378559023141861, Val Loss: 2.222705125808716, Val Acc: 89.84139730977715\n",
      "Epoch: [8/20], Step: [501/623], Learning Rate: 1.0000000000000003e-10, Train Loss: 0.13962465524673462, Val Loss: 2.222097635269165, Val Acc: 89.70086328046577\n",
      "Epoch: [8/20], Step: [601/623], Learning Rate: 1.0000000000000003e-10, Train Loss: 0.13909071683883667, Val Loss: 2.220871925354004, Val Acc: 89.96185504918691\n",
      "Epoch: [8/20], Train Acc: 96.59187873312253\n",
      "Epoch: [9/20], Step: [101/623], Learning Rate: 1.0000000000000003e-11, Train Loss: 0.1159110814332962, Val Loss: 2.220170259475708, Val Acc: 89.8012447299739\n",
      "Epoch: [9/20], Step: [201/623], Learning Rate: 1.0000000000000003e-11, Train Loss: 0.12073283642530441, Val Loss: 2.219142436981201, Val Acc: 89.86147359967877\n",
      "Epoch: [9/20], Step: [301/623], Learning Rate: 1.0000000000000003e-11, Train Loss: 0.12379294633865356, Val Loss: 2.2185230255126953, Val Acc: 89.8012447299739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9/20], Step: [401/623], Learning Rate: 1.0000000000000003e-11, Train Loss: 0.12003212422132492, Val Loss: 2.2176501750946045, Val Acc: 89.86147359967877\n",
      "Epoch: [9/20], Step: [501/623], Learning Rate: 1.0000000000000003e-11, Train Loss: 0.12127676606178284, Val Loss: 2.217550277709961, Val Acc: 89.84139730977715\n",
      "Epoch: [9/20], Step: [601/623], Learning Rate: 1.0000000000000003e-11, Train Loss: 0.1202932596206665, Val Loss: 2.216719388961792, Val Acc: 89.84139730977715\n",
      "Epoch: [9/20], Train Acc: 97.26446820257993\n",
      "Epoch: [10/20], Step: [101/623], Learning Rate: 1.0000000000000002e-12, Train Loss: 0.0992848202586174, Val Loss: 2.2160398960113525, Val Acc: 90.00200762899016\n",
      "Training stopped at epoch 10, iteration 101\n",
      "Learning rate: annealing from 0.001 - Best Val Acc: 89.98193133908853\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 50\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_anneal2 = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_anneal2.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Learning Rate: {}, Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), learning_rate, np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "#                 if bad_iter >= 3:\n",
    "#                     learning_rate /= 10\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "    learning_rate /= 10\n",
    "\n",
    "print('Learning rate: annealing from 0.001 - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_acc_lr01)\n",
    "plt.plot(val_acc_lr001)\n",
    "plt.plot(val_acc_lr0001)\n",
    "plt.plot(val_acc_anneal)\n",
    "plt.plot(val_acc_anneal2)\n",
    "plt.title('Learning rate tuning - Validation Acc')\n",
    "plt.legend(['0.01', '0.001', '0.0001', 'annealing from 0.01', 'annealing from 0.001'], loc='lower right')\n",
    "# plt.show()\n",
    "plt.savefig('Assignment_1/training_curve_lr.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annealing from 0.001 achieved best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using the best parameter set and evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 19923\n",
      "Val dataset size is 4981\n",
      "Test dataset size is 24678\n",
      "Total number of tokens in train dataset is 2466030\n",
      "Epoch: [1/50], Step: [101/623], Learning Rate: 0.001, Train Loss: 2.521371603012085, Val Loss: 2.8943724632263184, Val Acc: 49.92973298534431\n",
      "Epoch: [1/50], Step: [201/623], Learning Rate: 0.001, Train Loss: 1.9363017082214355, Val Loss: 2.653021812438965, Val Acc: 51.9975908452118\n",
      "Epoch: [1/50], Step: [301/623], Learning Rate: 0.001, Train Loss: 1.566598653793335, Val Loss: 2.5825133323669434, Val Acc: 60.02810680586228\n",
      "Epoch: [1/50], Step: [401/623], Learning Rate: 0.001, Train Loss: 1.3574897050857544, Val Loss: 2.561321258544922, Val Acc: 65.00702670146556\n",
      "Epoch: [1/50], Step: [501/623], Learning Rate: 0.001, Train Loss: 1.2241930961608887, Val Loss: 2.5463428497314453, Val Acc: 66.5729773137924\n",
      "Epoch: [1/50], Step: [601/623], Learning Rate: 0.001, Train Loss: 1.1305886507034302, Val Loss: 2.530771017074585, Val Acc: 71.49166833969082\n",
      "Epoch: [1/50], Train Acc: 74.51689002660243\n",
      "Epoch: [2/50], Step: [101/623], Learning Rate: 0.0001, Train Loss: 0.6197088956832886, Val Loss: 2.506803035736084, Val Acc: 75.58723147962256\n",
      "Epoch: [2/50], Step: [201/623], Learning Rate: 0.0001, Train Loss: 0.5988404154777527, Val Loss: 2.4829227924346924, Val Acc: 78.07669142742421\n",
      "Epoch: [2/50], Step: [301/623], Learning Rate: 0.0001, Train Loss: 0.5791880488395691, Val Loss: 2.4579062461853027, Val Acc: 79.66271832965268\n",
      "Epoch: [2/50], Step: [401/623], Learning Rate: 0.0001, Train Loss: 0.5617265105247498, Val Loss: 2.433445692062378, Val Acc: 81.54988958040555\n",
      "Epoch: [2/50], Step: [501/623], Learning Rate: 0.0001, Train Loss: 0.5445803999900818, Val Loss: 2.4116084575653076, Val Acc: 82.5938566552901\n",
      "Epoch: [2/50], Step: [601/623], Learning Rate: 0.0001, Train Loss: 0.5276610851287842, Val Loss: 2.3910961151123047, Val Acc: 83.61774744027304\n",
      "Epoch: [2/50], Train Acc: 85.79029262661246\n",
      "Epoch: [3/50], Step: [101/623], Learning Rate: 1e-05, Train Loss: 0.4071909785270691, Val Loss: 2.370278835296631, Val Acc: 84.50110419594459\n",
      "Epoch: [3/50], Step: [201/623], Learning Rate: 1e-05, Train Loss: 0.39496561884880066, Val Loss: 2.354506254196167, Val Acc: 84.8223248343706\n",
      "Epoch: [3/50], Step: [301/623], Learning Rate: 1e-05, Train Loss: 0.3824262320995331, Val Loss: 2.3411755561828613, Val Acc: 85.08331660309175\n",
      "Epoch: [3/50], Step: [401/623], Learning Rate: 1e-05, Train Loss: 0.37281179428100586, Val Loss: 2.328503370285034, Val Acc: 85.94659706886168\n",
      "Epoch: [3/50], Step: [501/623], Learning Rate: 1e-05, Train Loss: 0.36423107981681824, Val Loss: 2.3183586597442627, Val Acc: 86.28789399718931\n",
      "Epoch: [3/50], Step: [601/623], Learning Rate: 1e-05, Train Loss: 0.3561791479587555, Val Loss: 2.3097920417785645, Val Acc: 86.7295723750251\n",
      "Epoch: [3/50], Train Acc: 89.69532700898459\n",
      "Epoch: [4/50], Step: [101/623], Learning Rate: 1.0000000000000002e-06, Train Loss: 0.2853858470916748, Val Loss: 2.2998530864715576, Val Acc: 86.93033527404135\n",
      "Epoch: [4/50], Step: [201/623], Learning Rate: 1.0000000000000002e-06, Train Loss: 0.2854475975036621, Val Loss: 2.29333233833313, Val Acc: 87.111021883156\n",
      "Epoch: [4/50], Step: [301/623], Learning Rate: 1.0000000000000002e-06, Train Loss: 0.2787605822086334, Val Loss: 2.286771297454834, Val Acc: 87.41216623168039\n",
      "Epoch: [4/50], Step: [401/623], Learning Rate: 1.0000000000000002e-06, Train Loss: 0.2780374586582184, Val Loss: 2.2817704677581787, Val Acc: 87.61292913069664\n",
      "Epoch: [4/50], Step: [501/623], Learning Rate: 1.0000000000000002e-06, Train Loss: 0.27575764060020447, Val Loss: 2.276667356491089, Val Acc: 87.75346316000802\n",
      "Epoch: [4/50], Step: [601/623], Learning Rate: 1.0000000000000002e-06, Train Loss: 0.2738822102546692, Val Loss: 2.272067070007324, Val Acc: 87.75346316000802\n",
      "Epoch: [4/50], Train Acc: 91.89379109571851\n",
      "Epoch: [5/50], Step: [101/623], Learning Rate: 1.0000000000000002e-07, Train Loss: 0.23895888030529022, Val Loss: 2.2672877311706543, Val Acc: 88.17506524794219\n",
      "Epoch: [5/50], Step: [201/623], Learning Rate: 1.0000000000000002e-07, Train Loss: 0.23229703307151794, Val Loss: 2.263381004333496, Val Acc: 88.19514153784381\n",
      "Epoch: [5/50], Step: [301/623], Learning Rate: 1.0000000000000002e-07, Train Loss: 0.22886061668395996, Val Loss: 2.260248899459839, Val Acc: 88.39590443686006\n",
      "Epoch: [5/50], Step: [401/623], Learning Rate: 1.0000000000000002e-07, Train Loss: 0.23130638897418976, Val Loss: 2.2574572563171387, Val Acc: 88.61674362577796\n",
      "Epoch: [5/50], Step: [501/623], Learning Rate: 1.0000000000000002e-07, Train Loss: 0.22951743006706238, Val Loss: 2.254678726196289, Val Acc: 88.89781168440072\n",
      "Epoch: [5/50], Step: [601/623], Learning Rate: 1.0000000000000002e-07, Train Loss: 0.22772490978240967, Val Loss: 2.251817464828491, Val Acc: 88.89781168440072\n",
      "Epoch: [5/50], Train Acc: 93.40962706419715\n",
      "Epoch: [6/50], Step: [101/623], Learning Rate: 1.0000000000000002e-08, Train Loss: 0.18687523901462555, Val Loss: 2.247990608215332, Val Acc: 88.93796426420397\n",
      "Epoch: [6/50], Step: [201/623], Learning Rate: 1.0000000000000002e-08, Train Loss: 0.19414184987545013, Val Loss: 2.2461042404174805, Val Acc: 88.99819313390886\n",
      "Epoch: [6/50], Step: [301/623], Learning Rate: 1.0000000000000002e-08, Train Loss: 0.1918024718761444, Val Loss: 2.2440524101257324, Val Acc: 88.79743023489259\n",
      "Epoch: [6/50], Step: [401/623], Learning Rate: 1.0000000000000002e-08, Train Loss: 0.19215697050094604, Val Loss: 2.2421040534973145, Val Acc: 88.91788797430235\n",
      "Epoch: [6/50], Step: [501/623], Learning Rate: 1.0000000000000002e-08, Train Loss: 0.19189900159835815, Val Loss: 2.2398860454559326, Val Acc: 89.27926119253162\n",
      "Epoch: [6/50], Step: [601/623], Learning Rate: 1.0000000000000002e-08, Train Loss: 0.1921752244234085, Val Loss: 2.2382054328918457, Val Acc: 89.3394900622365\n",
      "Epoch: [6/50], Train Acc: 94.57912964914922\n",
      "Epoch: [7/50], Step: [101/623], Learning Rate: 1.0000000000000003e-09, Train Loss: 0.1748884916305542, Val Loss: 2.236614465713501, Val Acc: 89.45994780164625\n",
      "Epoch: [7/50], Step: [201/623], Learning Rate: 1.0000000000000003e-09, Train Loss: 0.1700519174337387, Val Loss: 2.2348062992095947, Val Acc: 89.45994780164625\n",
      "Epoch: [7/50], Step: [301/623], Learning Rate: 1.0000000000000003e-09, Train Loss: 0.16589929163455963, Val Loss: 2.2332425117492676, Val Acc: 89.50010038144951\n",
      "Epoch: [7/50], Step: [401/623], Learning Rate: 1.0000000000000003e-09, Train Loss: 0.16332753002643585, Val Loss: 2.2320642471313477, Val Acc: 89.52017667135114\n",
      "Epoch: [7/50], Step: [501/623], Learning Rate: 1.0000000000000003e-09, Train Loss: 0.16655032336711884, Val Loss: 2.2309184074401855, Val Acc: 89.419795221843\n",
      "Epoch: [7/50], Step: [601/623], Learning Rate: 1.0000000000000003e-09, Train Loss: 0.165555939078331, Val Loss: 2.22944712638855, Val Acc: 89.58040554105601\n",
      "Epoch: [7/50], Train Acc: 95.62314912412789\n",
      "Epoch: [8/50], Step: [101/623], Learning Rate: 1.0000000000000003e-10, Train Loss: 0.13868969678878784, Val Loss: 2.2278409004211426, Val Acc: 89.48002409154788\n",
      "Epoch: [8/50], Step: [201/623], Learning Rate: 1.0000000000000003e-10, Train Loss: 0.14305734634399414, Val Loss: 2.2266154289245605, Val Acc: 89.50010038144951\n",
      "Epoch: [8/50], Step: [301/623], Learning Rate: 1.0000000000000003e-10, Train Loss: 0.1421254724264145, Val Loss: 2.225579261779785, Val Acc: 89.54025296125276\n",
      "Epoch: [8/50], Step: [401/623], Learning Rate: 1.0000000000000003e-10, Train Loss: 0.14318619668483734, Val Loss: 2.2250213623046875, Val Acc: 89.50010038144951\n",
      "Epoch: [8/50], Step: [501/623], Learning Rate: 1.0000000000000003e-10, Train Loss: 0.14302818477153778, Val Loss: 2.2238712310791016, Val Acc: 89.56032925115439\n",
      "Epoch: [8/50], Step: [601/623], Learning Rate: 1.0000000000000003e-10, Train Loss: 0.14344505965709686, Val Loss: 2.2230753898620605, Val Acc: 89.54025296125276\n",
      "Epoch: [8/50], Train Acc: 96.3358931887768\n",
      "Epoch: [9/50], Step: [101/623], Learning Rate: 1.0000000000000003e-11, Train Loss: 0.12070652842521667, Val Loss: 2.2225286960601807, Val Acc: 89.68078699056414\n",
      "Epoch: [9/50], Step: [201/623], Learning Rate: 1.0000000000000003e-11, Train Loss: 0.12292064726352692, Val Loss: 2.2220540046691895, Val Acc: 89.48002409154788\n",
      "Epoch: [9/50], Step: [301/623], Learning Rate: 1.0000000000000003e-11, Train Loss: 0.12207265943288803, Val Loss: 2.22092604637146, Val Acc: 89.39971893194138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9/50], Step: [401/623], Learning Rate: 1.0000000000000003e-11, Train Loss: 0.12247323989868164, Val Loss: 2.220038414001465, Val Acc: 89.37964264203976\n",
      "Epoch: [9/50], Step: [501/623], Learning Rate: 1.0000000000000003e-11, Train Loss: 0.1239626333117485, Val Loss: 2.219496965408325, Val Acc: 89.60048183095763\n",
      "Epoch: [9/50], Step: [601/623], Learning Rate: 1.0000000000000003e-11, Train Loss: 0.124024897813797, Val Loss: 2.2187366485595703, Val Acc: 89.50010038144951\n",
      "Epoch: [9/50], Train Acc: 97.03357928022888\n",
      "Epoch: [10/50], Step: [101/623], Learning Rate: 1.0000000000000002e-12, Train Loss: 0.10223643481731415, Val Loss: 2.2187798023223877, Val Acc: 89.31941377233487\n",
      "Epoch: [10/50], Step: [201/623], Learning Rate: 1.0000000000000002e-12, Train Loss: 0.09924912452697754, Val Loss: 2.2173407077789307, Val Acc: 89.6406344107609\n",
      "Epoch: [10/50], Step: [301/623], Learning Rate: 1.0000000000000002e-12, Train Loss: 0.10283908247947693, Val Loss: 2.216705560684204, Val Acc: 89.52017667135114\n",
      "Epoch: [10/50], Step: [401/623], Learning Rate: 1.0000000000000002e-12, Train Loss: 0.10410089790821075, Val Loss: 2.2160773277282715, Val Acc: 89.62055812085927\n",
      "Epoch: [10/50], Step: [501/623], Learning Rate: 1.0000000000000002e-12, Train Loss: 0.10491768270730972, Val Loss: 2.215925931930542, Val Acc: 89.62055812085927\n",
      "Epoch: [10/50], Step: [601/623], Learning Rate: 1.0000000000000002e-12, Train Loss: 0.10687616467475891, Val Loss: 2.215519666671753, Val Acc: 89.70086328046577\n",
      "Training stopped at epoch 10, iteration 601\n",
      "Learning rate: annealing from 0.001 - Best Val Acc: 89.68078699056414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ji/anaconda3/envs/nlp/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BagOfWords. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 87.470621606289\n"
     ]
    }
   ],
   "source": [
    "def tokenize_dataset(dataset, n = 1):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        stop_words = set(stopwords.words('english')) \n",
    "        tokens = [w for w in tokens if not w in stop_words] \n",
    "        if n > 1:\n",
    "            ngrams_gn = ngrams(tokens, n)\n",
    "            tokens = []\n",
    "            for gram in ngrams_gn:\n",
    "                tokens.append(gram)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "# # test set tokens\n",
    "# print (\"Tokenizing test data\")\n",
    "# test_data_tokens, _ = tokenize_dataset(test_data_rmdup)\n",
    "# pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "\n",
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens.p\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n",
    "\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size = 10000):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "#     if rm_stopwords:\n",
    "#         stop_words = set(stopwords.words('english')) \n",
    "#         all_tokens = [w for w in all_tokens if not w in stop_words] \n",
    "#     if lemmatize:\n",
    "#         lmtzr = WordNetLemmatizer()\n",
    "#         all_tokens = [lmtzr.lemmatize(w) for w in all_tokens]\n",
    "#     if stem:\n",
    "#         ps = PorterStemmer()\n",
    "#         all_tokens = [ps.stem(w) for w in all_tokens]\n",
    "#     if n > 1:\n",
    "#         all_tokens = ngrams(all_tokens, n)\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = 20000)\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens, token2id_map = token2id)\n",
    "val_data_indices = token2index_dataset(val_data_tokens, token2id_map = token2id)\n",
    "test_data_indices = token2index_dataset(test_data_tokens, token2id_map = token2id)\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 300\n",
    "\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_label_rmdup)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=data_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "emb_dim = 50\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acc_best = 0\n",
    "val_acc_anneal = []\n",
    "ep_iter_best = [0,0]\n",
    "bad_iter = 0\n",
    "break_flag = False\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, model)\n",
    "            val_acc_anneal.append(val_acc)\n",
    "#             test_acc, test_loss = test_model(test_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Learning Rate: {}, Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), learning_rate, np.mean(train_loss), val_loss, val_acc))\n",
    "            # early stopping patience = 10\n",
    "            if bad_iter < 10:\n",
    "                if val_acc >= val_acc_best:\n",
    "                    val_acc_best = val_acc\n",
    "                    ep_iter_best = [epoch+1, i+1]\n",
    "                    bad_iter = 0\n",
    "                    torch.save(model.state_dict(), 'Assignment_1/bow-epoch{}-iter{}'.format(epoch + 1, i+1))\n",
    "                elif val_acc < val_acc_best:\n",
    "                    bad_iter += 1\n",
    "#                 if bad_iter >= 3:\n",
    "#                     learning_rate /= 10\n",
    "            elif bad_iter >= 10:\n",
    "                print('Training stopped at epoch {}, iteration {}'.format(epoch+1, i+1))\n",
    "                break_flag = True\n",
    "                break\n",
    "    if break_flag:\n",
    "        break\n",
    "    train_acc, train_loss = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc: {}'.format(epoch+1, num_epochs, train_acc))\n",
    "    learning_rate /= 10\n",
    "\n",
    "print('Learning rate: annealing from 0.001 - Best Val Acc: {}'.format(val_acc_best))\n",
    "    #print(train_loss)\n",
    "## load the best model\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "model.load_state_dict(torch.load('Assignment_1/bow-epoch{}-iter{}'.format(ep_iter_best[0], ep_iter_best[1])))\n",
    "torch.save(model, 'Assignment_1/bow-epoch{}-iter{}_whole'.format(ep_iter_best[0], ep_iter_best[1]))\n",
    "\n",
    "model = torch.load('Assignment_1/bow-epoch{}-iter{}_whole'.format(ep_iter_best[0], ep_iter_best[1]))\n",
    "model.eval()\n",
    "test_acc, _ = test_model(test_loader, model)\n",
    "print('Test Acc: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "## extracting cases from val set\n",
    "model = torch.load('Assignment_1/bow-epoch{}-iter{}_whole'.format(ep_iter_best[0], ep_iter_best[1]))\n",
    "model.eval()\n",
    "correct = 0\n",
    "for data, lengths, labels in val_loader:\n",
    "    data_batch, length_batch, label_batch = data, lengths, labels\n",
    "    outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "    predicted = outputs.max(1, keepdim=True)[1]\n",
    "    correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    if correct >= 3 and len(data_batch) - correct >= 3:\n",
    "        break\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0],\n",
       "        [ 1],\n",
       "        [ 2],\n",
       "        [ 4],\n",
       "        [ 5],\n",
       "        [ 6],\n",
       "        [ 7],\n",
       "        [ 8],\n",
       "        [ 9],\n",
       "        [10],\n",
       "        [11],\n",
       "        [12],\n",
       "        [13],\n",
       "        [14],\n",
       "        [15],\n",
       "        [16],\n",
       "        [17],\n",
       "        [18],\n",
       "        [19],\n",
       "        [21],\n",
       "        [22],\n",
       "        [23],\n",
       "        [24],\n",
       "        [25],\n",
       "        [26],\n",
       "        [28],\n",
       "        [29],\n",
       "        [30],\n",
       "        [31]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_batch.eq(predicted.view_as(label_batch)).nonzero()\n",
    "# correct: 0,1,2\n",
    "# incorrect: 3,20,27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_token, _ = tokenize_dataset(train_data_all_rmdup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_indices = token2index_dataset(train_all_token, token2id_map = token2id)\n",
    "train_all_dataset = NewsGroupDataset(train_all_indices, train_label_all_rmdup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_idx = [0,1,2]\n",
    "incorrect_idx = [3,20,27]\n",
    "cor_data_idx = []\n",
    "incor_data_idx = []\n",
    "for i in correct_idx:\n",
    "    for j in range(len(train_all_indices)):\n",
    "        if np.array_equal(data_batch[i].data.numpy()[:len(train_all_indices[j])], train_all_indices[j]):\n",
    "            cor_data_idx.append(j)\n",
    "\n",
    "for i in incorrect_idx:\n",
    "    for j in range(len(train_all_indices)):\n",
    "        if np.array_equal(data_batch[i].data.numpy()[:len(train_all_indices[j])], train_all_indices[j]):\n",
    "            incor_data_idx.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4668, 824, 15788]\n",
      "[15736, 21518, 12980]\n"
     ]
    }
   ],
   "source": [
    "print(cor_data_idx)\n",
    "print(incor_data_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I never heard of the book, nor care to read it, but the movie I will probably see many times. This film is unforgettable with perhaps the richest imagery I have ever seen in a movie. It was as if I was looking at paintings many times, which I think was the idea. Terrific movie, story, actors, and cinematography. Full of profound emotions from every angle. Although I am not particularly fond of romance movies, I loved this and was deeply moved by Winona Ryders plea to her father toward the end. Mr. Irons deserved an award for his performance and Close was never better. \n",
      "575\n",
      "A trio of buddies, sergeants all in the British Army, carouse & brawl their way across Imperial India. Intensely loyal to each other, they meet their greatest & most deadly challenge when they encounter the resurgence of a hideous cult & its demented, implacable guru. Now they must rely on the lowliest servant of the regiment, the water carrier GUNGA DIN, to save scores of the Queens soldiers from certain massacre. Based more on The Three Musketeers than Kiplings classic poem, this is a wonderful adventure epic - a worthy entry in Hollywoods Golden Year of 1939. Filled with suspense & humor, while keeping the romantic interludes to the barest minimum, it grips the interest of the viewer and holds it right up to the (sentimental) conclusion. It is practically fruitless to discuss the performance nuances of the three stars, Cary Grant, Victor McLaglen & Douglas Fairbanks Jr., as they are really all thirds of a single organism - inseparable and, to all intents & purposes, indistinguishable. However, this diminishes nothing of the great fun in simply watching them have a glorious time. (Its interesting to note, parenthetically, that McLaglen boasted of a distinguished World War One military career; Fairbanks would have a sterling record in World War Two - mostly in clandestine affairs & earning himself no fewer than 4 honorary knighthoods after the conflict; while Grant reportedly worked undercover for British Intelligence, keeping an eye on Hollywood Nazi sympathizers.) The real acting laurels here should go to Sam Jaffe, heartbreaking in the title role. He infuses the humble man with radiant dignity & enormous courage, making the last line of Kiplings poem ring true. He is unforgettable. Montague Love is properly stalwart as the regimental major, whilst Eduardo Ciannelli is Evil Incarnate as the Thuggee guru. The rest of the cast, Joan Fontaine, Robert Coote, Lumsden Hare, are effective but have little to do. Movie mavens will recognize Cecil Kellaway in the tiny role of Miss Fontaines father. The film picks its villains well. The demonic Thuggee cult, worshipers of the hideous, blood-soaked Kali, Hindu goddess of destruction, was the bane of Indian life for 6 centuries, ritualistically strangling up to 30,000 victims a year. In 1840 the British military, in cooperation with a number of princely states, succeeded in ultimately suppressing the religion. Henceforth it would remain the stuff of novels & nightmares.\n",
      "2453\n",
      "After stopping by the movie store to find something to watch, we stumbled on this. It looked appealing from the summary, at least, so we gave it a try. And heres the kicker: the first 20 minutes are interesting! Its actually enjoyable! Oh, wait, spoke too soon. Somewhere in there, the movie took a disgusting turn into fundamental, right-wing Christian brain-washing. Not entirely sure what happens, but I think the screenplay writer found God somewhere in there, finished writing this script, and had no time to edit it because he had a KKK meeting to get to with his friends from the Westboro Church and his hood wasnt clean. Can they put warnings on this? I refuse to support this religious idiocy. Much like video games have rating systems, movies need some sort of symbol: maybe a small cross in the bottom corner to show us that a movie is going to take a turn for the worse. Unless you share sentiments with whatever moron came up with this story, and will have your Bible open in your lap while you watch this and plan on how youll convert your neighbors, dont waste your time. Its some of the worst junk thats come out in a very long time, and the radical religious nuts dont need anymore funding.\n",
      "1207\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "for i in (cor_data_idx):\n",
    "    print(train_data_all_rmdup[i])\n",
    "    print(len(train_data_all_rmdup[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(data_batch[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Although I am generally a proponent of the well-made film, I do not limit myself to films which escape those boundaries, and more often than not I do enjoy and admire films that successfully \"break the rules.\" And it is quite true that director Pasolini breaks the rules of established cinema. But it is also my opinion that he does not break them successfully or to any actual point. Pasolinis work is visually jarring, but this is less a matter of what is actually on the screen than how it is filmed, and the jumpiness of his films seem less a matter of artistic choice than the result of amateur cinematography. This is true of DECAMERON. Pasolini often preferred to use non-actors, and while many directors have done so with remarkable result, under Pasolinis direction his non-actors tend to remain non-actors. This is also true of DECAMERON. Pasolini quite often includes images designed to shock, offend, or otherwise disconcert the audience. Such elements can often be used with startling effect, but in Pasolinis hands such elements seldom seem to actually contribute anything to the film. This is also true of DECAMERON. I have been given to understand there are many people who like, even admire Pasolinis films. Even so, I have never actually met any of them, and I have never been able to read anything about Pasolini or his works that made the reason for such liking or admiration comprehensible to me. Judging him from his works alone, I am of the opinion that he was essentially an amateurish director who did not \"break the rules\" so much by choice as by lack of skill--and who was initially applauded by the intelligentsia of his day for \" existential boldness,\" thereby simply confirming him in bad habits as a film maker. I find his work tedious, unimpressive, and pretentious. And this, too, is true of DECAMERON. It is also, sadly, true of virtually every Pasolini film it has been my misfortune to endure.\n",
      "1929\n",
      "Rob Schneider is a famous comedian cause of his movements, facials and performances of \"not humans\". This time he is The Animal. Marvin is a loser who is trying to be a hero and one day, nobody takes a call from a man that gets attacked, so Marvin has to take this case and save the attacked man. But on his way to the crime scene, he crashes with his car and gets really damaged. He doesnt remember what happens and at the next ordinary day, his life is not same when he finds out that he has animal instincts. Of course, we got our female that our main character is trying to reach but his tryings, are useless. She is played by Collen Haskell. There are no negative characters. The negative character, is destiny if I could use this metaphor. Marvin should find out, how to become a normal human being again. By the way-his animal instincts, helps him in some situations. Schneiders performance is a so-so. The movie is so unreal that gets stupid at some moments but it is one of those movies, called mindless fun as I have written above. So watch it for the monkey style Rob Schneider but it is definitely not one of the best comedies ever or one of the best movies that Schneider appears in. He is a great comedian but this is not his best movie.\n",
      "1251\n",
      "\"Laugh, Clown Laugh\" released in 1928, stars the legendary Lon Chaney as a circus clown named Tito. Tito has raised a foundling (a young and beautiful Loretta Young) to adulthood and names her Simonetta. Tito has raised the girl in the circus life, and she has become an accomplished ballerina. While Chaney gives his usual great performance, I could not get past the fact that Tito, now well into middle age, has the hots for the young Simonetta. Although he is not her biological father, he has raised her like a daughter. That kind of \"ick\" factor permeates throughout the film. Tito competes for Simonettas affections with a young and handsome Count Luigi (Nils Asther). Simonetta clearly falls for the young man, but feels guilt about abandoning Tito (out of loyalty, not romantic love). The whole premise of the film is ridiculous, and I find it amazing that no one in the film tells Tito what a stupid old fool he is being (until he reveals it himself at the end). The film is noteworthy only because of Loretta Young, who would go on to have a great career. While I adore Chaneys brilliance as an actor, this whole film seems off to me and just downright creepy.\n",
      "1170\n"
     ]
    }
   ],
   "source": [
    "for i in (incor_data_idx):\n",
    "    print(train_data_all_rmdup[i])\n",
    "    print(len(train_data_all_rmdup[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
